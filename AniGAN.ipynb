{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4WIccyy4RRQ"
   },
   "source": [
    "# Dataset Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 22973\n",
      "All images ok!\n"
     ]
    }
   ],
   "source": [
    "# get all image paths and count\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "file_paths = glob('./images/*/*')\n",
    "n_files = len(file_paths)\n",
    "print(f'Total images: {n_files}')\n",
    "\n",
    "# check through each image\n",
    "from PIL import Image\n",
    "faulty_images = []\n",
    "for path in file_paths:\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        # size check\n",
    "        if img.size[0] < 1 or img.size[1] < 1:\n",
    "            print(path, f'invalid size {img.size}')\n",
    "            faulty_images.append(path)\n",
    "    except Exception as e:\n",
    "        # error reading image\n",
    "        print(path, e)\n",
    "        faulty_images.append(path)\n",
    "\n",
    "if len(faulty_images) == 0:\n",
    "    print('All images ok!')\n",
    "else:\n",
    "    print(f'Broken images found, deleting {len(faulty_images)} faulty images...')\n",
    "    for path in faulty_images:\n",
    "        os.remove(path)\n",
    "    print('Relisting images...')\n",
    "    file_paths = glob('./images/*/*')\n",
    "    n_files = len(file_paths)\n",
    "    print(f'Total images: {n_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-Pa9qUu4RRY"
   },
   "source": [
    "## Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tMzkbsTE4RRZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# '/media/zx02/HDD/DATA/anime_faces/images'\n",
    "\n",
    "def get_dataset(image_size, batch_size):\n",
    "    ds = tf.keras.preprocessing.image_dataset_from_directory('./images', label_mode=None,\n",
    "                                                             shuffle=True,\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             image_size=image_size)\n",
    "    return ds.map(lambda images: (images-127.5) / 127.5).prefetch(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sshomYa4RRZ"
   },
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rYiMqe6w4RRZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.keras.layers.ops import core as core_ops\n",
    "import numpy as np\n",
    "\n",
    "class PixelNormalization(Layer):\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.math.rsqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + 1e-8)\n",
    "    \n",
    "class Conv2DEqualized(Layer):\n",
    "    \n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1)\n",
    "    bias_initializer = tf.keras.initializers.Zeros()\n",
    "    \n",
    "    def __init__(self, n_kernels, kernel_size, padding, name):\n",
    "        super(Conv2DEqualized, self).__init__(name=name)\n",
    "        self.n_kernels = n_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.c = np.sqrt(2.0 / float(input_shape[-1] * self.kernel_size[0] * self.kernel_size[1]))\n",
    "        with tf.name_scope(self.name):\n",
    "            self.kernels = self.add_weight(\n",
    "                name='kernel',\n",
    "                shape=(*self.kernel_size, input_shape[-1], self.n_kernels),\n",
    "                initializer=Conv2DEqualized.kernel_initializer,\n",
    "                trainable=True)\n",
    "            self.bias = self.add_weight(\n",
    "              name='bias',\n",
    "              shape=(self.n_kernels,),\n",
    "              initializer=Conv2DEqualized.bias_initializer,\n",
    "              trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = tf.nn.conv2d(inputs, self.kernels * self.c, 1, self.padding.upper(), name='conv_op')\n",
    "            x = tf.nn.bias_add(x, self.bias, name='bias_op')\n",
    "            return x\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'n_kernels' : self.n_kernels,\n",
    "            'kernel_size' : self.kernel_size,\n",
    "            'padding' : self.padding,\n",
    "            'name' : self.name\n",
    "        }\n",
    "\n",
    "class DenseEqualized(Layer):\n",
    "\n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1)\n",
    "    bias_initializer = tf.keras.initializers.Zeros()\n",
    "    \n",
    "    def __init__(self, n_units, name):\n",
    "        super(DenseEqualized, self).__init__(name=name)\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.c = np.sqrt(2.0 / float(input_shape[-1]))\n",
    "        with tf.name_scope(self.name):\n",
    "            self.kernels = self.add_weight(\n",
    "                name='weight',\n",
    "                shape=(input_shape[-1], self.n_units),\n",
    "                initializer=DenseEqualized.kernel_initializer,\n",
    "                trainable=True)\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(self.n_units,),\n",
    "                initializer=DenseEqualized.bias_initializer,\n",
    "                trainable=True\n",
    "            )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            return core_ops.dense(inputs, self.kernels * self.c, self.bias)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'n_units' : self.n_units,\n",
    "            'name' : self.name\n",
    "        }\n",
    "    \n",
    "class FadeinMerge(Layer):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super(FadeinMerge, self).__init__(name=name)\n",
    "        \n",
    "    def build(self, input_shapes):\n",
    "        self.alpha = self.add_weight(name='alpha',\n",
    "                                     shape=tuple(),\n",
    "                                     initializer='zero',\n",
    "                                     trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return (1 - self.alpha) * inputs[0] + self.alpha * inputs[1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'name' : self.name\n",
    "        }\n",
    "    \n",
    "class MiniBatchStdDev(Layer):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super(MiniBatchStdDev, self).__init__(name=name)\n",
    "        self.group_size = 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.transpose(inputs, perm=[0, 3, 1, 2])\n",
    "        s = x.shape                                             # [NCHW]  Input shape.\n",
    "        y = tf.reshape(x, [self.group_size, -1, s[1], s[2], s[3]])   # [GMCHW] Split minibatch into M groups of size G.\n",
    "        y = tf.cast(y, tf.float32)                              # [GMCHW] Cast to FP32.\n",
    "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMCHW] Subtract mean over group.\n",
    "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MCHW]  Calc variance over group.\n",
    "        y = tf.sqrt(y + 1e-8)                                   # [MCHW]  Calc stddev over group.\n",
    "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
    "        y = tf.cast(y, x.dtype)                                 # [M111]  Cast back to original data type.\n",
    "        y = tf.tile(y, [self.group_size, 1, s[2], s[3]])        # [N1HW]  Replicate over group and pixels.\n",
    "        x = tf.concat([x, y], axis=1)                           # [NCHW]  Append as new fmap.\n",
    "        x = tf.transpose(x, perm=[0, 2, 3, 1])\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'name' : self.name\n",
    "        }\n",
    "    \n",
    "custom_objects = {\n",
    "    'PixelNormalization' : PixelNormalization,\n",
    "    'Conv2DEqualized' : Conv2DEqualized,\n",
    "    'DenseEqualized' : DenseEqualized,\n",
    "    'FadeinMerge' : FadeinMerge,\n",
    "    'MiniBatchStdDev' : MiniBatchStdDev,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5QQwgcM64RRa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Generator:\n",
    "    \n",
    "    fadein_alpha = None\n",
    "    model_no_fadein = None\n",
    "\n",
    "    def __init__(self, config={}):\n",
    "        # training params\n",
    "        self.config = config.copy()\n",
    "        self.initial_build()\n",
    "            \n",
    "    def initial_build(self):\n",
    "        if self.config['during_fadein']:\n",
    "            self.config['stage'] -= 1\n",
    "        n_stage = self.config['stage']\n",
    "        # normalize inputs\n",
    "        self.input = Input((512), name='gen/input')\n",
    "        x = PixelNormalization(name='gen/input_norm')(self.input)\n",
    "        x = DenseEqualized(512 * 4 * 4, name='gen/input_map')(x)\n",
    "        x = LeakyReLU(0.2, name='gen/input_map_act')(x)\n",
    "        x = PixelNormalization(name='gen/input_map_norm')(x)\n",
    "        x = Reshape((4, 4, 512))(x)\n",
    "        for stage in range(1, n_stage+1):\n",
    "            with tf.name_scope(f'stage_{stage}'):\n",
    "                if stage > 1:\n",
    "                    x = self.upscale_block(x, stage)\n",
    "                else:\n",
    "                    x = self.first_upscale_block(x)\n",
    "        x = self.to_rgb(x, is_identity=False)\n",
    "        self.model = Model(self.input, x)\n",
    "        if self.config['during_fadein']:\n",
    "            self.grow_with_fadein()\n",
    "        \n",
    "    def grow_with_fadein(self):\n",
    "        self.config['stage'] += 1\n",
    "        # skip 'to_rgb' layer to get the last Conv2D\n",
    "        old_model_bottom = self.model.layers[-2]\n",
    "        # build both residual & normal branches\n",
    "        with tf.name_scope(f'stage_{self.config[\"stage\"]}'):\n",
    "            x_merged, x = self.upscale_block(old_model_bottom.output, self.config['stage'], use_fadein=True)\n",
    "        # build model without fadein\n",
    "        self.model_no_fadein = Model(self.input, x)\n",
    "        # rebuild model with fadein\n",
    "        self.model = Model(self.input, x_merged)\n",
    "        self.find_fadein_alpha()\n",
    "    \n",
    "    def remove_fadein(self):\n",
    "        del self.model\n",
    "        self.model = self.model_no_fadein\n",
    "    \n",
    "    def first_upscale_block(self, x):\n",
    "        prefix = 'gen/stage1/'\n",
    "        x = Conv2DEqualized(512, (4, 4), padding='same', name=prefix+'conv1')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr1')(x)\n",
    "        x = PixelNormalization(name=prefix+'pn1')(x)\n",
    "        x = Conv2DEqualized(512, (3, 3), padding='same', name=prefix+'conv2')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr2')(x)\n",
    "        x = PixelNormalization(name=prefix+'pn2')(x)\n",
    "        return x\n",
    "    \n",
    "    def upscale_block(self, x, stage, use_fadein=False):\n",
    "        if stage > 4:\n",
    "            depth = int(512 / (2 ** (stage - 4)))\n",
    "        else:\n",
    "            depth = 512\n",
    "        prefix = f'gen/stage{stage}/'\n",
    "        x = UpSampling2D((2, 2), name=prefix+'us1')(x)\n",
    "        if use_fadein:\n",
    "            id_x = self.to_rgb(x, is_identity=True)\n",
    "        x = Conv2DEqualized(depth, (3, 3), padding='same', name=prefix+'conv1')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr1')(x)\n",
    "        x = PixelNormalization(name=prefix+'pn1')(x)\n",
    "        x = Conv2DEqualized(depth, (3, 3), padding='same', name=prefix+'conv2')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr2')(x)\n",
    "        x = PixelNormalization(name=prefix+'pn2')(x)\n",
    "        # additional steps if this block is being faded in\n",
    "        if use_fadein:\n",
    "            # to_rgb before addition\n",
    "            x = self.to_rgb(x, is_identity=False)\n",
    "            # add and return both the merged and original branches\n",
    "            merged_x = FadeinMerge(name='gen/fadein_merge')([id_x, x])\n",
    "            return merged_x, x\n",
    "        # return normally\n",
    "        return x\n",
    "    \n",
    "    def to_rgb(self, x, is_identity=False):\n",
    "        x = Conv2DEqualized(3, (1, 1), padding='same', name=f'gen/{\"id_\" if is_identity else \"\"}to_rgb')(x)\n",
    "        return x\n",
    "    \n",
    "    def set_fadein_alpha(self, alpha):\n",
    "        self.fadein_alpha.assign(alpha)\n",
    "        \n",
    "    def find_fadein_alpha(self):\n",
    "        layer = self.model.get_layer('gen/fadein_merge')\n",
    "        if layer:\n",
    "            self.fadein_alpha = layer.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1QVNB_L34RRa"
   },
   "outputs": [],
   "source": [
    "def reflow_layers(x_in, layers):\n",
    "    x = x_in\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "class Discriminator:\n",
    "    \n",
    "    fadein_alpha = None\n",
    "    model_no_fadein = None\n",
    "    \n",
    "    def __init__(self, config={}):\n",
    "        # training params\n",
    "        self.config = config.copy()\n",
    "        self.initial_build()\n",
    "            \n",
    "    def update_input(self):\n",
    "        stage = self.config['stage']\n",
    "        image_dim = int(2 ** (stage + 1))\n",
    "        self.input = Input((image_dim, image_dim, 3), name='dis/input')\n",
    "    \n",
    "    def initial_build(self):\n",
    "        if self.config['during_fadein']:\n",
    "            self.config['stage'] -= 1\n",
    "        n_stage = self.config['stage']\n",
    "        self.update_input()\n",
    "        x = self.from_rgb(self.input, n_stage, is_identity=False)\n",
    "        for stage in reversed(range(1, n_stage+1)):\n",
    "            with tf.name_scope(f'stage_{stage}'):\n",
    "                if stage > 1:\n",
    "                    x = self.downscale_block(x, stage)\n",
    "                else:\n",
    "                    x = self.last_block(x)\n",
    "        x = Flatten(name='dis/flatten')(x)\n",
    "        x = DenseEqualized(1, name='dis/dense')(x)\n",
    "        self.model = Model(self.input, x)\n",
    "        if self.config['during_fadein']:\n",
    "            self.grow_with_fadein()\n",
    "        \n",
    "    def grow_with_fadein(self):\n",
    "        ''' Perform fade-in growth of the discriminator.'''\n",
    "        self.config['stage'] += 1\n",
    "        self.update_input()\n",
    "        stage = self.config[\"stage\"]\n",
    "        # skip connection\n",
    "        id_x = AveragePooling2D((2, 2), name='dis/id_ap')(self.input)\n",
    "        id_x = self.from_rgb(id_x, stage, is_identity=True)\n",
    "        # main branch\n",
    "        x = self.from_rgb(self.input, stage, is_identity=False)\n",
    "        with tf.name_scope(f'stage_{stage}'):\n",
    "            x = self.downscale_block(x, stage)\n",
    "        # merge with alpha\n",
    "        merged_x = FadeinMerge(name='dis/fadein_merge')([id_x, x])\n",
    "        # build model without fadein to be used later\n",
    "        # skip: Conv2D(1x1), PixelNorm to get the first non 1x1 Conv2D\n",
    "        old_model_layers = self.model.layers[2:]\n",
    "        x = reflow_layers(x, old_model_layers)\n",
    "        self.model_no_fadein = Model(self.input, x)\n",
    "        # rebuild model with fadein\n",
    "        # attach new block\n",
    "        fadein_x = reflow_layers(merged_x, old_model_layers)\n",
    "        self.model = Model(self.input, fadein_x)\n",
    "        self.find_fadein_alpha()\n",
    "    \n",
    "    def remove_fadein(self):\n",
    "        self.model = self.model_no_fadein\n",
    "        self.model_no_fadein = None\n",
    "    \n",
    "    def last_block(self, x):\n",
    "        prefix = 'dis/stage1/'\n",
    "        x = MiniBatchStdDev(name=prefix + 'minibatch_std')(x)\n",
    "        x = Conv2DEqualized(512, (3, 3), padding='same', name=prefix+'conv1')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr1')(x)\n",
    "        x = Conv2DEqualized(512, (4, 4), padding='valid', name=prefix+'conv2')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr2')(x)\n",
    "        return x\n",
    "    \n",
    "    def downscale_block(self, x, stage):\n",
    "        if stage > 4:\n",
    "            ini_depth = int(512 / (2 ** (stage - 4)))\n",
    "            final_depth = ini_depth * 2\n",
    "        else:\n",
    "            ini_depth = final_depth = 512\n",
    "        prefix = f'dis/stage{stage}/'\n",
    "        x = Conv2DEqualized(ini_depth, (3, 3), padding='same', name=prefix+'conv1')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr1')(x)\n",
    "        x = Conv2DEqualized(final_depth , (3, 3), padding='same', name=prefix+'conv2')(x)\n",
    "        x = LeakyReLU(0.2, name=prefix+'lr2')(x)\n",
    "        x = AveragePooling2D((2, 2), name=prefix+'ap')(x)\n",
    "        return x\n",
    "    \n",
    "    def from_rgb(self, x, stage, is_identity):\n",
    "        if stage > 4:\n",
    "            depth = int(512 / (2 ** (stage - 4)))\n",
    "            if is_identity:\n",
    "                depth *= 2\n",
    "        else:\n",
    "            depth = 512\n",
    "        x = Conv2DEqualized(depth, (1, 1), padding='same', name=f'dis/{\"id_\" if is_identity else \"\"}from_rgb')(x)\n",
    "        return x\n",
    "    \n",
    "    def set_fadein_alpha(self, alpha):\n",
    "        self.fadein_alpha.assign(alpha)\n",
    "        \n",
    "    def find_fadein_alpha(self):\n",
    "        layer = self.model.get_layer('dis/fadein_merge')\n",
    "        if layer:\n",
    "            self.fadein_alpha = layer.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwfZF57e4RRb"
   },
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y4XzFZCi4RRb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def image_size_for_stage(stage):\n",
    "    dim = int(2 ** (stage + 1))\n",
    "    return (dim, dim)\n",
    "\n",
    "class WGAN(Model):\n",
    "    \n",
    "    epoch = 0\n",
    "    g_optimizer = Adam(1e-3, 0, 0.99)\n",
    "    d_optimizer = Adam(1e-3, 0, 0.99)\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config=None,\n",
    "        checkpoint_path=None\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        # note that config is shared among the generator and discriminator\n",
    "        loading_from_save = not config\n",
    "        if config:\n",
    "            self.config = config\n",
    "        else:\n",
    "            with open(os.path.join(checkpoint_path, 'config.json')) as f:\n",
    "                self.config = json.load(f)\n",
    "        self.generator = Generator(self.config)\n",
    "        self.discriminator = Discriminator(self.config)\n",
    "        if loading_from_save:\n",
    "            # prepass to initialize all vars\n",
    "            self.compile()\n",
    "            images = tf.random.normal((self.batch_size, *self.image_size, 3))\n",
    "            self.train_step(images)\n",
    "            # restore states from checkpoint\n",
    "            ckpt = self.create_checkpoint()\n",
    "            ckpt.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "    def create_checkpoint(self):\n",
    "        if self.during_fadein:\n",
    "                checkpoint = tf.train.Checkpoint(d_optimizer=self.d_optimizer, g_optimizer=self.g_optimizer,\n",
    "                                                 generator=self.generator_func, discriminator=self.discriminator_func,\n",
    "                                                 generator_no_fadein=self.generator.model_no_fadein, discriminator_no_fadein=self.discriminator.model_no_fadein)\n",
    "        else:\n",
    "            checkpoint = tf.train.Checkpoint(d_optimizer=self.d_optimizer, g_optimizer=self.g_optimizer,\n",
    "                                             generator=self.generator_func, discriminator=self.discriminator_func)\n",
    "        return checkpoint\n",
    "                \n",
    "    def compile(self):\n",
    "        super(WGAN, self).compile()\n",
    "        self.fadein_alpha = None\n",
    "        self.d_loss_fn = lambda real_logits, fake_logits: - tf.reduce_mean(real_logits) + tf.reduce_mean(fake_logits)\n",
    "        self.g_loss_fn = lambda gen_img_logits: -tf.reduce_mean(gen_img_logits)\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator_func(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2.0)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, 512)\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images from the latent vector\n",
    "            fake_images = self.generator_func(random_latent_vectors, training=True)\n",
    "            # Get the logits for the fake images\n",
    "            fake_logits = self.discriminator_func(fake_images, training=True)\n",
    "            # Get the logits for the real images\n",
    "            real_logits = self.discriminator_func(real_images, training=True)\n",
    "            # Calculate the discriminator loss using the fake and real image logits\n",
    "            d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "            # Calculate the gradient penalty\n",
    "            gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "            # Add the gradient penalty to the original discriminator loss\n",
    "            d_loss = d_cost + gp * 10.0\n",
    "\n",
    "        # Get the gradients w.r.t the discriminator loss\n",
    "        d_gradient = tape.gradient(d_loss, self.discriminator_func.trainable_variables)\n",
    "        # Update the weights of the discriminator using the discriminator optimizer\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(d_gradient, self.discriminator_func.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, 512))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator_func(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator_func(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator_func.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator_func.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss,\n",
    "                'mean_real_logits': tf.reduce_mean(real_logits),\n",
    "                'mean_fake_logits': tf.reduce_mean(fake_logits),\n",
    "                'mean_gen_logits': tf.reduce_mean(gen_img_logits)}\n",
    "    \n",
    "    @property\n",
    "    def generator_func(self):\n",
    "        return self.generator.model\n",
    "    \n",
    "    @property\n",
    "    def discriminator_func(self):\n",
    "        return self.discriminator.model\n",
    "    \n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.config['stage']\n",
    "    \n",
    "    @property\n",
    "    def image_size(self):\n",
    "        return image_size_for_stage(self.stage)\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        batch_sizes = [256, 256, 128, 64, 28, 12, 6, 3, 3]\n",
    "        return batch_sizes[self.stage-1]\n",
    "    \n",
    "    @property\n",
    "    def n_epochs(self):\n",
    "        epochs = [35, 45, 60, 75, 75, 30, 30, 30]\n",
    "        return epochs[self.stage]\n",
    "    \n",
    "    @property\n",
    "    def during_fadein(self):\n",
    "        return self.generator.model_no_fadein is not None and self.discriminator.model_no_fadein is not None\n",
    "    \n",
    "    def fit_n_epochs(self, epochs, initial_epoch=0):\n",
    "        self.fit(get_dataset(self.image_size, self.batch_size), epochs=epochs, callbacks=[WGANCallback()], initial_epoch=initial_epoch)\n",
    "\n",
    "    def initial_cycle(self, override_n_epochs=None):\n",
    "        n_epochs = override_n_epochs if override_n_epochs else self.n_epochs\n",
    "        self.fit_n_epochs(n_epochs)\n",
    "    \n",
    "    def one_growth_cycle(self, override_n_epochs=None):\n",
    "        n_epochs = override_n_epochs if override_n_epochs else self.n_epochs\n",
    "        # from tensorflow.keras.utils import plot_model\n",
    "        # the fadein_alpha will be updated by the callback after each batch\n",
    "        self.config['stage'] += 1\n",
    "        print(f'New stage {self.stage}')\n",
    "        # grow with fade-in\n",
    "        self.generator.grow_with_fadein()\n",
    "        self.discriminator.grow_with_fadein()\n",
    "        self.compile()\n",
    "        print('Model growth completed.')\n",
    "        # stabilise\n",
    "        print('Stabilising with fade-in.')\n",
    "        self.fit_n_epochs(n_epochs)\n",
    "        # remove fade-in\n",
    "        print('Removed fade-in and re-stabilising.')\n",
    "        self.generator.remove_fadein()\n",
    "        self.discriminator.remove_fadein()\n",
    "        self.compile()\n",
    "        # stabilise\n",
    "        self.fit_n_epochs(n_epochs)\n",
    "        \n",
    "    def set_fadein_alpha(self, alpha):\n",
    "        self.generator.set_fadein_alpha(alpha)\n",
    "        self.discriminator.set_fadein_alpha(alpha)\n",
    "        \n",
    "    def save(self, checkpoint_path:str=None):\n",
    "        # make folder\n",
    "        now = datetime.now()\n",
    "        if not checkpoint_path:\n",
    "            checkpoint_path = f'saves/save_{self.config[\"stage\"]}_' + now.strftime(f\"%d-%m-%Y_%H-%M\")\n",
    "        os.mkdir(checkpoint_path)\n",
    "        # save config\n",
    "        config = self.config.copy()\n",
    "        config['epoch'] = self.epoch\n",
    "        config['during_fadein'] = self.during_fadein\n",
    "        # create 'saves' dir if necessary\n",
    "        if checkpoint_path.startswith('saves/') and not os.path.isdir('saves'):\n",
    "            os.mkdir('saves')\n",
    "        with open(f'{checkpoint_path}/config.json', 'w') as f:\n",
    "            json.dump(config, f)\n",
    "        # save model\n",
    "        ckpt = self.create_checkpoint()\n",
    "        save_path = ckpt.save(os.path.join(checkpoint_path, 'ckpt'))\n",
    "        print(f'Model saved at {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANCallback(Callback):\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.batch = batch\n",
    "        # update fadein_alpha every epoch\n",
    "        if self.model.during_fadein:\n",
    "            progress = float(self.epoch) / self.model.n_epochs + float(batch) / float(22973 // self.model.batch_size)\n",
    "            self.model.set_fadein_alpha(progress)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.model.epoch = self.epoch = epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generate = True\n",
    "        # only generate samples sparingly on the first two stages\n",
    "        if self.model.stage < 3 and epoch % 3 != 0:\n",
    "            return\n",
    "        # generate samples\n",
    "        latent = tf.random.normal((10 * 10, 512))\n",
    "        images = self.model.generator_func(latent, training=False)\n",
    "        images = tf.clip_by_value((images + 1.0) / 2.0, 0.0, 1.0)\n",
    "        fig, ax = plt.subplots(10, 10, figsize=(30, 30))\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                ax[i][j].imshow(images[i * 10 + j])\n",
    "        fig.tight_layout()\n",
    "        # write to image\n",
    "        stage = self.model.stage\n",
    "        # make dir 'generated' if necessary\n",
    "        if not os.path.isdir('generated'):\n",
    "            os.mkdir('generated')\n",
    "        file_name = f'generated/stage{stage}_epoch{epoch}.png'\n",
    "        fig.savefig(file_name)\n",
    "        plt.close()\n",
    "        # save model every 10 epochs for stage > 4\n",
    "        if stage > 4 and epoch % 10 == 0 and epoch > 0:\n",
    "            self.model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "9JgDtqZq4RRc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22973 files belonging to 1 classes.\n",
      "Epoch 1/45\n",
      "90/90 [==============================] - 15s 143ms/step - d_loss: -0.8082 - g_loss: 4.5313 - mean_real_logits: -0.1254 - mean_fake_logits: -2.3730 - mean_gen_logits: -4.5313\n",
      "Epoch 2/45\n",
      "90/90 [==============================] - 14s 145ms/step - d_loss: -1.0502 - g_loss: 0.1936 - mean_real_logits: 1.5494 - mean_fake_logits: 0.1263 - mean_gen_logits: -0.1936\n",
      "Epoch 3/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.6266 - g_loss: -0.3914 - mean_real_logits: 1.6571 - mean_fake_logits: 0.8018 - mean_gen_logits: 0.3914\n",
      "Epoch 4/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.4600 - g_loss: -0.5893 - mean_real_logits: 1.5568 - mean_fake_logits: 0.9000 - mean_gen_logits: 0.5893\n",
      "Epoch 5/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3987 - g_loss: -0.8369 - mean_real_logits: 1.7038 - mean_fake_logits: 1.1430 - mean_gen_logits: 0.8369\n",
      "Epoch 6/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3523 - g_loss: -0.9894 - mean_real_logits: 1.8221 - mean_fake_logits: 1.3275 - mean_gen_logits: 0.9894\n",
      "Epoch 7/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3628 - g_loss: -1.0540 - mean_real_logits: 1.9075 - mean_fake_logits: 1.4028 - mean_gen_logits: 1.0540\n",
      "Epoch 8/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.3869 - g_loss: -1.0759 - mean_real_logits: 1.9530 - mean_fake_logits: 1.4312 - mean_gen_logits: 1.0759\n",
      "Epoch 9/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.4580 - g_loss: -1.0809 - mean_real_logits: 1.9568 - mean_fake_logits: 1.3646 - mean_gen_logits: 1.0809\n",
      "Epoch 10/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.4737 - g_loss: -1.1162 - mean_real_logits: 2.0111 - mean_fake_logits: 1.4026 - mean_gen_logits: 1.1162\n",
      "Epoch 11/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.4765 - g_loss: -1.0855 - mean_real_logits: 1.9609 - mean_fake_logits: 1.3481 - mean_gen_logits: 1.0855\n",
      "Epoch 12/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5082 - g_loss: -1.0093 - mean_real_logits: 1.9091 - mean_fake_logits: 1.2665 - mean_gen_logits: 1.0093\n",
      "Epoch 13/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5251 - g_loss: -0.9635 - mean_real_logits: 1.8533 - mean_fake_logits: 1.1951 - mean_gen_logits: 0.9635\n",
      "Epoch 14/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5039 - g_loss: -0.9338 - mean_real_logits: 1.8032 - mean_fake_logits: 1.1624 - mean_gen_logits: 0.9338\n",
      "Epoch 15/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5262 - g_loss: -0.8821 - mean_real_logits: 1.7672 - mean_fake_logits: 1.1010 - mean_gen_logits: 0.8821\n",
      "Epoch 16/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5437 - g_loss: -0.8117 - mean_real_logits: 1.7263 - mean_fake_logits: 1.0436 - mean_gen_logits: 0.8117\n",
      "Epoch 17/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5652 - g_loss: -0.8240 - mean_real_logits: 1.7410 - mean_fake_logits: 1.0438 - mean_gen_logits: 0.8240\n",
      "Epoch 18/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5767 - g_loss: -0.8002 - mean_real_logits: 1.7305 - mean_fake_logits: 1.0222 - mean_gen_logits: 0.8002\n",
      "Epoch 19/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5574 - g_loss: -0.8011 - mean_real_logits: 1.7462 - mean_fake_logits: 1.0556 - mean_gen_logits: 0.8011\n",
      "Epoch 20/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5889 - g_loss: -0.8549 - mean_real_logits: 1.7709 - mean_fake_logits: 1.0540 - mean_gen_logits: 0.8549\n",
      "Epoch 21/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.5976 - g_loss: -0.8887 - mean_real_logits: 1.7911 - mean_fake_logits: 1.0683 - mean_gen_logits: 0.8887\n",
      "Epoch 22/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.6032 - g_loss: -0.9080 - mean_real_logits: 1.8094 - mean_fake_logits: 1.0847 - mean_gen_logits: 0.9080\n",
      "Epoch 23/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.5925 - g_loss: -0.9157 - mean_real_logits: 1.8133 - mean_fake_logits: 1.1014 - mean_gen_logits: 0.9157\n",
      "Epoch 24/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.5880 - g_loss: -0.9369 - mean_real_logits: 1.8321 - mean_fake_logits: 1.1237 - mean_gen_logits: 0.9369\n",
      "Epoch 25/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5861 - g_loss: -0.9881 - mean_real_logits: 1.8621 - mean_fake_logits: 1.1612 - mean_gen_logits: 0.9881\n",
      "Epoch 26/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5763 - g_loss: -0.9983 - mean_real_logits: 1.8744 - mean_fake_logits: 1.1858 - mean_gen_logits: 0.9983\n",
      "Epoch 27/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5739 - g_loss: -1.0604 - mean_real_logits: 1.9177 - mean_fake_logits: 1.2346 - mean_gen_logits: 1.0604\n",
      "Epoch 28/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.5797 - g_loss: -1.0982 - mean_real_logits: 1.9275 - mean_fake_logits: 1.2436 - mean_gen_logits: 1.0982\n",
      "Epoch 29/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5488 - g_loss: -1.1519 - mean_real_logits: 1.9891 - mean_fake_logits: 1.3365 - mean_gen_logits: 1.1519\n",
      "Epoch 30/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.5413 - g_loss: -1.2076 - mean_real_logits: 2.0057 - mean_fake_logits: 1.3647 - mean_gen_logits: 1.2076\n",
      "Epoch 31/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5377 - g_loss: -1.2774 - mean_real_logits: 2.0594 - mean_fake_logits: 1.4247 - mean_gen_logits: 1.2774\n",
      "Epoch 32/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.5187 - g_loss: -1.3685 - mean_real_logits: 2.1349 - mean_fake_logits: 1.5186 - mean_gen_logits: 1.3685\n",
      "Epoch 33/45\n",
      "90/90 [==============================] - 14s 145ms/step - d_loss: -0.4900 - g_loss: -1.4874 - mean_real_logits: 2.2292 - mean_fake_logits: 1.6454 - mean_gen_logits: 1.4874\n",
      "Epoch 34/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.4929 - g_loss: -1.5890 - mean_real_logits: 2.3036 - mean_fake_logits: 1.7183 - mean_gen_logits: 1.5890\n",
      "Epoch 35/45\n",
      "90/90 [==============================] - 14s 142ms/step - d_loss: -0.4587 - g_loss: -1.6790 - mean_real_logits: 2.3940 - mean_fake_logits: 1.8431 - mean_gen_logits: 1.6790\n",
      "Epoch 36/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.4614 - g_loss: -1.8156 - mean_real_logits: 2.4736 - mean_fake_logits: 1.9224 - mean_gen_logits: 1.8156\n",
      "Epoch 37/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.4066 - g_loss: -1.8706 - mean_real_logits: 2.5485 - mean_fake_logits: 2.0514 - mean_gen_logits: 1.8706\n",
      "Epoch 38/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.4094 - g_loss: -1.9849 - mean_real_logits: 2.5980 - mean_fake_logits: 2.1001 - mean_gen_logits: 1.9849\n",
      "Epoch 39/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3752 - g_loss: -2.0627 - mean_real_logits: 2.6742 - mean_fake_logits: 2.2079 - mean_gen_logits: 2.0627\n",
      "Epoch 40/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3472 - g_loss: -2.1117 - mean_real_logits: 2.6920 - mean_fake_logits: 2.2559 - mean_gen_logits: 2.1117\n",
      "Epoch 41/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.3279 - g_loss: -2.1805 - mean_real_logits: 2.7348 - mean_fake_logits: 2.3176 - mean_gen_logits: 2.1805\n",
      "Epoch 42/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.3172 - g_loss: -2.1666 - mean_real_logits: 2.7470 - mean_fake_logits: 2.3405 - mean_gen_logits: 2.1666\n",
      "Epoch 43/45\n",
      "90/90 [==============================] - 14s 145ms/step - d_loss: -0.2682 - g_loss: -2.2722 - mean_real_logits: 2.7564 - mean_fake_logits: 2.3988 - mean_gen_logits: 2.2722\n",
      "Epoch 44/45\n",
      "90/90 [==============================] - 14s 143ms/step - d_loss: -0.2225 - g_loss: -2.2268 - mean_real_logits: 2.6823 - mean_fake_logits: 2.3671 - mean_gen_logits: 2.2268\n",
      "Epoch 45/45\n",
      "90/90 [==============================] - 14s 144ms/step - d_loss: -0.2062 - g_loss: -2.1188 - mean_real_logits: 2.5454 - mean_fake_logits: 2.2451 - mean_gen_logits: 2.1188\n",
      "Model saved at save[1]_26-02-2021_01-00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "gan = WGAN({ 'stage' : 1, 'during_fadein' : False })\n",
    "gan.compile()\n",
    "gan.initial_cycle()\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gan.one_growth_cycle()\n",
    "gan.during_fadein = False\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = WGAN(checkpoint_path='save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New stage 3\n",
      "Model growth completed.\n",
      "Stabilising with fade-in.\n",
      "Found 22973 files belonging to 1 classes.\n",
      "Epoch 1/60\n",
      "180/180 [==============================] - 45s 225ms/step - d_loss: -3.8574 - g_loss: 2.9772 - mean_real_logits: 5.1577 - mean_fake_logits: 0.4985 - mean_gen_logits: -2.9772\n",
      "Epoch 2/60\n",
      "180/180 [==============================] - 40s 219ms/step - d_loss: -3.1951 - g_loss: 1.4349 - mean_real_logits: 3.9226 - mean_fake_logits: 0.2341 - mean_gen_logits: -1.4349\n",
      "Epoch 3/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -2.8792 - g_loss: 0.0100 - mean_real_logits: 4.8079 - mean_fake_logits: 1.5316 - mean_gen_logits: -0.0100 \n",
      "Epoch 4/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -2.3975 - g_loss: -0.0141 - mean_real_logits: 3.7275 - mean_fake_logits: 0.9987 - mean_gen_logits: 0.0141\n",
      "Epoch 5/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -2.2968 - g_loss: 1.1168 - mean_real_logits: 3.6323 - mean_fake_logits: 0.8879 - mean_gen_logits: -1.1168\n",
      "Epoch 6/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.9549 - g_loss: 6.5244 - mean_real_logits: -3.1164 - mean_fake_logits: -5.3713 - mean_gen_logits: -6.5244\n",
      "Epoch 7/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.8815 - g_loss: 2.9641 - mean_real_logits: 0.6822 - mean_fake_logits: -1.5004 - mean_gen_logits: -2.9641\n",
      "Epoch 8/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.6902 - g_loss: 1.8410 - mean_real_logits: 1.3648 - mean_fake_logits: -0.5536 - mean_gen_logits: -1.8410\n",
      "Epoch 9/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.6416 - g_loss: 1.7947 - mean_real_logits: 1.8537 - mean_fake_logits: -0.0545 - mean_gen_logits: -1.7947\n",
      "Epoch 10/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.5740 - g_loss: 2.3026 - mean_real_logits: 0.6149 - mean_fake_logits: -1.1749 - mean_gen_logits: -2.3026\n",
      "Epoch 11/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.5810 - g_loss: 1.8891 - mean_real_logits: 1.6378 - mean_fake_logits: -0.2415 - mean_gen_logits: -1.8891\n",
      "Epoch 12/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.5399 - g_loss: 2.5927 - mean_real_logits: 0.0178 - mean_fake_logits: -1.7285 - mean_gen_logits: -2.5927\n",
      "Epoch 13/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.6201 - g_loss: 1.8476 - mean_real_logits: 1.3911 - mean_fake_logits: -0.4874 - mean_gen_logits: -1.8476\n",
      "Epoch 14/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.4133 - g_loss: 2.5191 - mean_real_logits: 0.1440 - mean_fake_logits: -1.4550 - mean_gen_logits: -2.5191\n",
      "Epoch 15/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.4414 - g_loss: 3.1924 - mean_real_logits: -0.1617 - mean_fake_logits: -1.8444 - mean_gen_logits: -3.1924\n",
      "Epoch 16/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3574 - g_loss: 2.2600 - mean_real_logits: 0.1729 - mean_fake_logits: -1.3644 - mean_gen_logits: -2.2600\n",
      "Epoch 17/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.4434 - g_loss: 2.9379 - mean_real_logits: -0.0768 - mean_fake_logits: -1.7531 - mean_gen_logits: -2.9379\n",
      "Epoch 18/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2721 - g_loss: 2.7551 - mean_real_logits: -0.3483 - mean_fake_logits: -1.7864 - mean_gen_logits: -2.7551\n",
      "Epoch 19/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.3728 - g_loss: 3.2071 - mean_real_logits: -0.4106 - mean_fake_logits: -2.0372 - mean_gen_logits: -3.2071\n",
      "Epoch 20/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2030 - g_loss: 6.7495 - mean_real_logits: -4.5433 - mean_fake_logits: -5.9130 - mean_gen_logits: -6.7495\n",
      "Epoch 21/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3179 - g_loss: 5.4861 - mean_real_logits: -2.8517 - mean_fake_logits: -4.3659 - mean_gen_logits: -5.4861\n",
      "Epoch 22/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2003 - g_loss: 5.8551 - mean_real_logits: -3.5361 - mean_fake_logits: -4.8854 - mean_gen_logits: -5.8551\n",
      "Epoch 23/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3160 - g_loss: 6.4712 - mean_real_logits: -4.0182 - mean_fake_logits: -5.5225 - mean_gen_logits: -6.4712\n",
      "Epoch 24/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2536 - g_loss: 6.2514 - mean_real_logits: -3.8669 - mean_fake_logits: -5.2966 - mean_gen_logits: -6.2514\n",
      "Epoch 25/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1747 - g_loss: 6.2959 - mean_real_logits: -3.8824 - mean_fake_logits: -5.2342 - mean_gen_logits: -6.2959\n",
      "Epoch 26/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1513 - g_loss: 6.1681 - mean_real_logits: -4.1216 - mean_fake_logits: -5.4189 - mean_gen_logits: -6.1681\n",
      "Epoch 27/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2295 - g_loss: 6.6229 - mean_real_logits: -4.1379 - mean_fake_logits: -5.5621 - mean_gen_logits: -6.6229\n",
      "Epoch 28/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1766 - g_loss: 8.6428 - mean_real_logits: -6.3134 - mean_fake_logits: -7.6791 - mean_gen_logits: -8.6428\n",
      "Epoch 29/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2381 - g_loss: 9.3587 - mean_real_logits: -7.0395 - mean_fake_logits: -8.4697 - mean_gen_logits: -9.3587\n",
      "Epoch 30/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2213 - g_loss: 10.8534 - mean_real_logits: -8.5121 - mean_fake_logits: -9.9182 - mean_gen_logits: -10.8534\n",
      "Epoch 31/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1490 - g_loss: 10.9503 - mean_real_logits: -8.9024 - mean_fake_logits: -10.1917 - mean_gen_logits: -10.9503\n",
      "Epoch 32/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2854 - g_loss: 9.3217 - mean_real_logits: -6.9217 - mean_fake_logits: -8.3879 - mean_gen_logits: -9.3217\n",
      "Epoch 33/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.1992 - g_loss: 10.2488 - mean_real_logits: -7.9887 - mean_fake_logits: -9.3558 - mean_gen_logits: -10.2488\n",
      "Epoch 34/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1104 - g_loss: 9.9751 - mean_real_logits: -7.6089 - mean_fake_logits: -8.8980 - mean_gen_logits: -9.9751\n",
      "Epoch 35/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1729 - g_loss: 14.5150 - mean_real_logits: -12.3058 - mean_fake_logits: -13.6753 - mean_gen_logits: -14.5150\n",
      "Epoch 36/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.0926 - g_loss: 12.5627 - mean_real_logits: -10.2129 - mean_fake_logits: -11.4919 - mean_gen_logits: -12.5627\n",
      "Epoch 37/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.0952 - g_loss: 13.0875 - mean_real_logits: -11.3248 - mean_fake_logits: -12.5664 - mean_gen_logits: -13.0875\n",
      "Epoch 38/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2417 - g_loss: 10.7007 - mean_real_logits: -8.3999 - mean_fake_logits: -9.8157 - mean_gen_logits: -10.7007\n",
      "Epoch 39/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.0359 - g_loss: 12.5656 - mean_real_logits: -10.0446 - mean_fake_logits: -11.3797 - mean_gen_logits: -12.5656\n",
      "Epoch 40/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1495 - g_loss: 17.4454 - mean_real_logits: -15.5223 - mean_fake_logits: -16.8777 - mean_gen_logits: -17.4454\n",
      "Epoch 41/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.0891 - g_loss: 14.0229 - mean_real_logits: -11.7753 - mean_fake_logits: -13.0413 - mean_gen_logits: -14.0229\n",
      "Epoch 42/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2907 - g_loss: 14.1366 - mean_real_logits: -12.0234 - mean_fake_logits: -13.4800 - mean_gen_logits: -14.1366\n",
      "Epoch 43/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1154 - g_loss: 13.7615 - mean_real_logits: -11.4504 - mean_fake_logits: -12.7433 - mean_gen_logits: -13.7615\n",
      "Epoch 44/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1737 - g_loss: 16.0614 - mean_real_logits: -14.0079 - mean_fake_logits: -15.3663 - mean_gen_logits: -16.0614\n",
      "Epoch 45/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.0670 - g_loss: 15.8434 - mean_real_logits: -13.7042 - mean_fake_logits: -14.9287 - mean_gen_logits: -15.8434\n",
      "Epoch 46/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3388 - g_loss: 14.6622 - mean_real_logits: -12.4939 - mean_fake_logits: -14.0120 - mean_gen_logits: -14.6622\n",
      "Epoch 47/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2338 - g_loss: 14.5991 - mean_real_logits: -12.1868 - mean_fake_logits: -13.6017 - mean_gen_logits: -14.5991\n",
      "Epoch 48/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2305 - g_loss: 17.9703 - mean_real_logits: -15.7516 - mean_fake_logits: -17.1612 - mean_gen_logits: -17.9703\n",
      "Epoch 49/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2527 - g_loss: 17.1543 - mean_real_logits: -15.0156 - mean_fake_logits: -16.4320 - mean_gen_logits: -17.1543\n",
      "Epoch 50/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.1432 - g_loss: 15.3730 - mean_real_logits: -13.2506 - mean_fake_logits: -14.5450 - mean_gen_logits: -15.3730\n",
      "Epoch 51/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2508 - g_loss: 14.1649 - mean_real_logits: -11.8745 - mean_fake_logits: -13.2869 - mean_gen_logits: -14.1649\n",
      "Epoch 52/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2184 - g_loss: 14.6455 - mean_real_logits: -12.4289 - mean_fake_logits: -13.8334 - mean_gen_logits: -14.6455\n",
      "Epoch 53/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.2417 - g_loss: 13.3786 - mean_real_logits: -11.2773 - mean_fake_logits: -12.6903 - mean_gen_logits: -13.3786\n",
      "Epoch 54/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.1300 - g_loss: 13.9979 - mean_real_logits: -11.8452 - mean_fake_logits: -13.1429 - mean_gen_logits: -13.9979\n",
      "Epoch 55/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3140 - g_loss: 13.5784 - mean_real_logits: -11.3851 - mean_fake_logits: -12.8593 - mean_gen_logits: -13.5784\n",
      "Epoch 56/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.1805 - g_loss: 13.6530 - mean_real_logits: -11.5955 - mean_fake_logits: -12.9217 - mean_gen_logits: -13.6530\n",
      "Epoch 57/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.3049 - g_loss: 13.4991 - mean_real_logits: -11.2550 - mean_fake_logits: -12.7244 - mean_gen_logits: -13.4991\n",
      "Epoch 58/60\n",
      "180/180 [==============================] - 40s 218ms/step - d_loss: -1.2215 - g_loss: 12.3767 - mean_real_logits: -10.2000 - mean_fake_logits: -11.5849 - mean_gen_logits: -12.3767\n",
      "Epoch 59/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.0792 - g_loss: 10.5971 - mean_real_logits: -8.4821 - mean_fake_logits: -9.7381 - mean_gen_logits: -10.5971\n",
      "Epoch 60/60\n",
      "180/180 [==============================] - 40s 217ms/step - d_loss: -1.1540 - g_loss: 7.5559 - mean_real_logits: -5.4343 - mean_fake_logits: -6.7621 - mean_gen_logits: -7.5559\n",
      "Removed fade-in and re-stabilising.\n",
      "Found 22973 files belonging to 1 classes.\n",
      "Epoch 1/60\n",
      "180/180 [==============================] - 40s 211ms/step - d_loss: -1.1334 - g_loss: 13.9182 - mean_real_logits: -12.1857 - mean_fake_logits: -13.4094 - mean_gen_logits: -13.9182\n",
      "Epoch 2/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.2296 - g_loss: 13.1547 - mean_real_logits: -11.3284 - mean_fake_logits: -12.6402 - mean_gen_logits: -13.1547\n",
      "Epoch 3/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.1923 - g_loss: 12.7285 - mean_real_logits: -10.9305 - mean_fake_logits: -12.2050 - mean_gen_logits: -12.7285\n",
      "Epoch 4/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.1497 - g_loss: 12.2796 - mean_real_logits: -10.5104 - mean_fake_logits: -11.7409 - mean_gen_logits: -12.2796\n",
      "Epoch 5/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.1116 - g_loss: 12.3608 - mean_real_logits: -10.5730 - mean_fake_logits: -11.7619 - mean_gen_logits: -12.3608\n",
      "Epoch 6/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.1403 - g_loss: 12.3982 - mean_real_logits: -10.6718 - mean_fake_logits: -11.8891 - mean_gen_logits: -12.3982\n",
      "Epoch 7/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -1.1005 - g_loss: 11.7582 - mean_real_logits: -10.0621 - mean_fake_logits: -11.2369 - mean_gen_logits: -11.7582\n",
      "Epoch 8/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.1174 - g_loss: 11.5422 - mean_real_logits: -9.8275 - mean_fake_logits: -11.0205 - mean_gen_logits: -11.5422\n",
      "Epoch 9/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.0645 - g_loss: 11.5272 - mean_real_logits: -9.8769 - mean_fake_logits: -11.0143 - mean_gen_logits: -11.5272\n",
      "Epoch 10/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -1.0279 - g_loss: 11.1710 - mean_real_logits: -9.5053 - mean_fake_logits: -10.6040 - mean_gen_logits: -11.1710\n",
      "Epoch 11/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.0580 - g_loss: 10.9245 - mean_real_logits: -9.2876 - mean_fake_logits: -10.4168 - mean_gen_logits: -10.9245\n",
      "Epoch 12/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.0318 - g_loss: 11.0787 - mean_real_logits: -9.4728 - mean_fake_logits: -10.5734 - mean_gen_logits: -11.0787\n",
      "Epoch 13/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.0262 - g_loss: 10.4827 - mean_real_logits: -8.8583 - mean_fake_logits: -9.9546 - mean_gen_logits: -10.4827\n",
      "Epoch 14/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9916 - g_loss: 10.4030 - mean_real_logits: -8.8347 - mean_fake_logits: -9.8938 - mean_gen_logits: -10.4030\n",
      "Epoch 15/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -1.0006 - g_loss: 10.4910 - mean_real_logits: -8.8959 - mean_fake_logits: -9.9628 - mean_gen_logits: -10.4910\n",
      "Epoch 16/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9795 - g_loss: 10.0380 - mean_real_logits: -8.5063 - mean_fake_logits: -9.5519 - mean_gen_logits: -10.0380\n",
      "Epoch 17/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9770 - g_loss: 10.2411 - mean_real_logits: -8.6876 - mean_fake_logits: -9.7312 - mean_gen_logits: -10.2411\n",
      "Epoch 18/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9652 - g_loss: 9.8385 - mean_real_logits: -8.3069 - mean_fake_logits: -9.3359 - mean_gen_logits: -9.8385\n",
      "Epoch 19/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9711 - g_loss: 9.6480 - mean_real_logits: -8.1815 - mean_fake_logits: -9.2161 - mean_gen_logits: -9.6480\n",
      "Epoch 20/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9524 - g_loss: 9.7776 - mean_real_logits: -8.2576 - mean_fake_logits: -9.2720 - mean_gen_logits: -9.7776\n",
      "Epoch 21/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9424 - g_loss: 9.3651 - mean_real_logits: -7.8837 - mean_fake_logits: -8.8887 - mean_gen_logits: -9.3651\n",
      "Epoch 22/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9382 - g_loss: 9.1362 - mean_real_logits: -7.6836 - mean_fake_logits: -8.6834 - mean_gen_logits: -9.1362\n",
      "Epoch 23/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9292 - g_loss: 9.1298 - mean_real_logits: -7.6637 - mean_fake_logits: -8.6557 - mean_gen_logits: -9.1298\n",
      "Epoch 24/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.9368 - g_loss: 8.6378 - mean_real_logits: -7.1894 - mean_fake_logits: -8.1869 - mean_gen_logits: -8.6378\n",
      "Epoch 25/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9148 - g_loss: 8.9520 - mean_real_logits: -7.4958 - mean_fake_logits: -8.4703 - mean_gen_logits: -8.9520\n",
      "Epoch 26/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9168 - g_loss: 9.0406 - mean_real_logits: -7.5964 - mean_fake_logits: -8.5746 - mean_gen_logits: -9.0406\n",
      "Epoch 27/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9237 - g_loss: 9.0209 - mean_real_logits: -7.5912 - mean_fake_logits: -8.5763 - mean_gen_logits: -9.0209\n",
      "Epoch 28/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.9084 - g_loss: 8.6292 - mean_real_logits: -7.2288 - mean_fake_logits: -8.1966 - mean_gen_logits: -8.6292\n",
      "Epoch 29/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8895 - g_loss: 8.5383 - mean_real_logits: -7.0933 - mean_fake_logits: -8.0413 - mean_gen_logits: -8.5383\n",
      "Epoch 30/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8923 - g_loss: 8.6642 - mean_real_logits: -7.2776 - mean_fake_logits: -8.2279 - mean_gen_logits: -8.6642\n",
      "Epoch 31/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8823 - g_loss: 8.7191 - mean_real_logits: -7.3233 - mean_fake_logits: -8.2627 - mean_gen_logits: -8.7191\n",
      "Epoch 32/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8729 - g_loss: 8.6546 - mean_real_logits: -7.2725 - mean_fake_logits: -8.2029 - mean_gen_logits: -8.6546\n",
      "Epoch 33/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8785 - g_loss: 8.2802 - mean_real_logits: -6.8883 - mean_fake_logits: -7.8242 - mean_gen_logits: -8.2802\n",
      "Epoch 34/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8696 - g_loss: 8.4609 - mean_real_logits: -7.0964 - mean_fake_logits: -8.0214 - mean_gen_logits: -8.4609\n",
      "Epoch 35/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.8661 - g_loss: 7.8581 - mean_real_logits: -6.5154 - mean_fake_logits: -7.4360 - mean_gen_logits: -7.8581\n",
      "Epoch 36/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8595 - g_loss: 7.8983 - mean_real_logits: -6.5405 - mean_fake_logits: -7.4548 - mean_gen_logits: -7.8983\n",
      "Epoch 37/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8566 - g_loss: 8.2734 - mean_real_logits: -6.9495 - mean_fake_logits: -7.8601 - mean_gen_logits: -8.2734\n",
      "Epoch 38/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8336 - g_loss: 7.8366 - mean_real_logits: -6.5305 - mean_fake_logits: -7.4164 - mean_gen_logits: -7.8366\n",
      "Epoch 39/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8442 - g_loss: 7.8078 - mean_real_logits: -6.4722 - mean_fake_logits: -7.3683 - mean_gen_logits: -7.8078\n",
      "Epoch 40/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8434 - g_loss: 7.8203 - mean_real_logits: -6.5049 - mean_fake_logits: -7.4021 - mean_gen_logits: -7.8203\n",
      "Epoch 41/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8154 - g_loss: 8.0406 - mean_real_logits: -6.7239 - mean_fake_logits: -7.5922 - mean_gen_logits: -8.0406\n",
      "Epoch 42/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8318 - g_loss: 7.7896 - mean_real_logits: -6.5049 - mean_fake_logits: -7.3886 - mean_gen_logits: -7.7896\n",
      "Epoch 43/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8310 - g_loss: 7.4339 - mean_real_logits: -6.1221 - mean_fake_logits: -7.0055 - mean_gen_logits: -7.4339\n",
      "Epoch 44/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.8236 - g_loss: 7.5335 - mean_real_logits: -6.2354 - mean_fake_logits: -7.1099 - mean_gen_logits: -7.5335\n",
      "Epoch 45/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8083 - g_loss: 7.6945 - mean_real_logits: -6.4108 - mean_fake_logits: -7.2698 - mean_gen_logits: -7.6945\n",
      "Epoch 46/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8325 - g_loss: 7.5722 - mean_real_logits: -6.3192 - mean_fake_logits: -7.2035 - mean_gen_logits: -7.5722\n",
      "Epoch 47/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8028 - g_loss: 7.2531 - mean_real_logits: -5.9731 - mean_fake_logits: -6.8260 - mean_gen_logits: -7.2531\n",
      "Epoch 48/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.8139 - g_loss: 7.3553 - mean_real_logits: -6.0860 - mean_fake_logits: -6.9502 - mean_gen_logits: -7.3553\n",
      "Epoch 49/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.8128 - g_loss: 7.2992 - mean_real_logits: -6.0175 - mean_fake_logits: -6.8812 - mean_gen_logits: -7.2992\n",
      "Epoch 50/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8006 - g_loss: 7.1738 - mean_real_logits: -5.9240 - mean_fake_logits: -6.7746 - mean_gen_logits: -7.1738\n",
      "Epoch 51/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8016 - g_loss: 7.0116 - mean_real_logits: -5.7785 - mean_fake_logits: -6.6294 - mean_gen_logits: -7.0116\n",
      "Epoch 52/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8152 - g_loss: 6.9833 - mean_real_logits: -5.7170 - mean_fake_logits: -6.5820 - mean_gen_logits: -6.9833\n",
      "Epoch 53/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7978 - g_loss: 6.9397 - mean_real_logits: -5.7079 - mean_fake_logits: -6.5552 - mean_gen_logits: -6.9397\n",
      "Epoch 54/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.8034 - g_loss: 7.0278 - mean_real_logits: -5.7843 - mean_fake_logits: -6.6364 - mean_gen_logits: -7.0278\n",
      "Epoch 55/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7948 - g_loss: 7.0512 - mean_real_logits: -5.8348 - mean_fake_logits: -6.6778 - mean_gen_logits: -7.0512\n",
      "Epoch 56/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7988 - g_loss: 6.9018 - mean_real_logits: -5.6818 - mean_fake_logits: -6.5287 - mean_gen_logits: -6.9018\n",
      "Epoch 57/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7947 - g_loss: 6.7848 - mean_real_logits: -5.5548 - mean_fake_logits: -6.3980 - mean_gen_logits: -6.7848\n",
      "Epoch 58/60\n",
      "180/180 [==============================] - 39s 212ms/step - d_loss: -0.7818 - g_loss: 6.4638 - mean_real_logits: -5.2452 - mean_fake_logits: -6.0735 - mean_gen_logits: -6.4638\n",
      "Epoch 59/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7848 - g_loss: 6.6288 - mean_real_logits: -5.4429 - mean_fake_logits: -6.2752 - mean_gen_logits: -6.6288\n",
      "Epoch 60/60\n",
      "180/180 [==============================] - 39s 211ms/step - d_loss: -0.7698 - g_loss: 6.3010 - mean_real_logits: -5.0900 - mean_fake_logits: -5.9060 - mean_gen_logits: -6.3010\n",
      "Model saved at save_3_26-02-2021_08-54\n"
     ]
    }
   ],
   "source": [
    "gan.one_growth_cycle()\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = WGAN(checkpoint_path='save_3_26-02-2021_08-54')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============] - 140s 388ms/step - d_loss: -1.6330 - g_loss: 2.8704 - mean_real_logits: -0.0865 - mean_fake_logits: -1.9256 - mean_gen_logits: -2.8704\n",
      "Epoch 46/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6519 - g_loss: 2.6099 - mean_real_logits: 0.1658 - mean_fake_logits: -1.6600 - mean_gen_logits: -2.6099\n",
      "Epoch 47/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6562 - g_loss: 3.2190 - mean_real_logits: -0.3480 - mean_fake_logits: -2.1825 - mean_gen_logits: -3.2190\n",
      "Epoch 48/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7163 - g_loss: 2.9739 - mean_real_logits: -0.0486 - mean_fake_logits: -1.9469 - mean_gen_logits: -2.9739\n",
      "Epoch 49/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7257 - g_loss: 2.6391 - mean_real_logits: 0.1894 - mean_fake_logits: -1.7245 - mean_gen_logits: -2.6391\n",
      "Epoch 50/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7490 - g_loss: 3.1361 - mean_real_logits: -0.2053 - mean_fake_logits: -2.1465 - mean_gen_logits: -3.1361\n",
      "Epoch 51/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6707 - g_loss: 4.2427 - mean_real_logits: -1.3792 - mean_fake_logits: -3.2471 - mean_gen_logits: -4.2427\n",
      "Epoch 52/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6796 - g_loss: 3.2536 - mean_real_logits: -0.4706 - mean_fake_logits: -2.3208 - mean_gen_logits: -3.2536\n",
      "Epoch 53/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6683 - g_loss: 3.4142 - mean_real_logits: -0.5832 - mean_fake_logits: -2.4176 - mean_gen_logits: -3.4142\n",
      "Epoch 54/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7037 - g_loss: 3.0267 - mean_real_logits: -0.2000 - mean_fake_logits: -2.0781 - mean_gen_logits: -3.0267\n",
      "Epoch 55/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7439 - g_loss: 2.5873 - mean_real_logits: 0.2830 - mean_fake_logits: -1.6416 - mean_gen_logits: -2.5873\n",
      "Epoch 56/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6780 - g_loss: 3.9515 - mean_real_logits: -1.1359 - mean_fake_logits: -2.9841 - mean_gen_logits: -3.9515\n",
      "Epoch 57/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6891 - g_loss: 2.8085 - mean_real_logits: 0.0441 - mean_fake_logits: -1.8130 - mean_gen_logits: -2.8085\n",
      "Epoch 58/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7500 - g_loss: 3.1097 - mean_real_logits: -0.2044 - mean_fake_logits: -2.1472 - mean_gen_logits: -3.1097\n",
      "Epoch 59/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6892 - g_loss: 3.4010 - mean_real_logits: -0.6274 - mean_fake_logits: -2.4932 - mean_gen_logits: -3.4010\n",
      "Epoch 60/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6970 - g_loss: 3.4738 - mean_real_logits: -0.6785 - mean_fake_logits: -2.5418 - mean_gen_logits: -3.4738\n",
      "Epoch 61/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7230 - g_loss: 3.3241 - mean_real_logits: -0.5309 - mean_fake_logits: -2.4281 - mean_gen_logits: -3.3241\n",
      "Epoch 62/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6981 - g_loss: 2.0237 - mean_real_logits: 0.7809 - mean_fake_logits: -1.0827 - mean_gen_logits: -2.0237\n",
      "Epoch 63/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7197 - g_loss: 2.7668 - mean_real_logits: 0.0317 - mean_fake_logits: -1.8544 - mean_gen_logits: -2.7668\n",
      "Epoch 64/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7158 - g_loss: 2.4888 - mean_real_logits: 0.3021 - mean_fake_logits: -1.5908 - mean_gen_logits: -2.4888\n",
      "Epoch 65/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6963 - g_loss: 3.2137 - mean_real_logits: -0.4287 - mean_fake_logits: -2.2969 - mean_gen_logits: -3.2137\n",
      "Epoch 66/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7139 - g_loss: 3.3077 - mean_real_logits: -0.5484 - mean_fake_logits: -2.4356 - mean_gen_logits: -3.3077\n",
      "Epoch 67/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6903 - g_loss: 2.7165 - mean_real_logits: 0.0616 - mean_fake_logits: -1.8040 - mean_gen_logits: -2.7165\n",
      "Epoch 68/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7019 - g_loss: 2.7595 - mean_real_logits: 0.0536 - mean_fake_logits: -1.8174 - mean_gen_logits: -2.7595\n",
      "Epoch 69/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7058 - g_loss: 2.6854 - mean_real_logits: 0.0761 - mean_fake_logits: -1.7991 - mean_gen_logits: -2.6854\n",
      "Epoch 70/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6915 - g_loss: 2.4701 - mean_real_logits: 0.2501 - mean_fake_logits: -1.6109 - mean_gen_logits: -2.4701\n",
      "Epoch 71/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.6877 - g_loss: 2.5065 - mean_real_logits: 0.2343 - mean_fake_logits: -1.6232 - mean_gen_logits: -2.5065\n",
      "Epoch 72/75\n",
      "359/359 [==============================] - 140s 389ms/step - d_loss: -1.6934 - g_loss: 2.4292 - mean_real_logits: 0.3194 - mean_fake_logits: -1.5408 - mean_gen_logits: -2.4292\n",
      "Epoch 73/75\n",
      "359/359 [==============================] - 140s 389ms/step - d_loss: -1.6942 - g_loss: 2.9163 - mean_real_logits: -0.2171 - mean_fake_logits: -2.0801 - mean_gen_logits: -2.9163\n",
      "Epoch 74/75\n",
      "359/359 [==============================] - 140s 388ms/step - d_loss: -1.7157 - g_loss: 2.6482 - mean_real_logits: 0.1443 - mean_fake_logits: -1.7395 - mean_gen_logits: -2.6482\n",
      "Epoch 75/75\n",
      "359/359 [==============================] - 140s 389ms/step - d_loss: -1.7274 - g_loss: 3.1359 - mean_real_logits: -0.3265 - mean_fake_logits: -2.2412 - mean_gen_logits: -3.1359\n",
      "Removed fade-in and re-stabilising.\n",
      "Found 22973 files belonging to 1 classes.\n",
      "Epoch 1/75\n",
      "359/359 [==============================] - 138s 379ms/step - d_loss: -1.5182 - g_loss: 1.1313 - mean_real_logits: 1.1294 - mean_fake_logits: -0.5243 - mean_gen_logits: -1.1313\n",
      "Epoch 2/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.5942 - g_loss: 1.1712 - mean_real_logits: 1.2306 - mean_fake_logits: -0.4853 - mean_gen_logits: -1.1712\n",
      "Epoch 3/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.5892 - g_loss: 1.5737 - mean_real_logits: 0.7585 - mean_fake_logits: -0.9528 - mean_gen_logits: -1.5737\n",
      "Epoch 4/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.5422 - g_loss: 1.5501 - mean_real_logits: 0.7613 - mean_fake_logits: -0.9005 - mean_gen_logits: -1.5501\n",
      "Epoch 5/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.5339 - g_loss: 1.3135 - mean_real_logits: 0.9886 - mean_fake_logits: -0.6612 - mean_gen_logits: -1.3135\n",
      "Epoch 6/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.5144 - g_loss: 1.4935 - mean_real_logits: 0.7658 - mean_fake_logits: -0.8644 - mean_gen_logits: -1.4935\n",
      "Epoch 7/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4926 - g_loss: 1.8013 - mean_real_logits: 0.4500 - mean_fake_logits: -1.1570 - mean_gen_logits: -1.8013\n",
      "Epoch 8/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4676 - g_loss: 1.8009 - mean_real_logits: 0.4224 - mean_fake_logits: -1.1580 - mean_gen_logits: -1.8009\n",
      "Epoch 9/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4749 - g_loss: 1.8105 - mean_real_logits: 0.3802 - mean_fake_logits: -1.2096 - mean_gen_logits: -1.8105\n",
      "Epoch 10/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4654 - g_loss: 1.7760 - mean_real_logits: 0.4093 - mean_fake_logits: -1.1670 - mean_gen_logits: -1.7760\n",
      "Epoch 11/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4420 - g_loss: 1.2467 - mean_real_logits: 0.9333 - mean_fake_logits: -0.6174 - mean_gen_logits: -1.2467\n",
      "Epoch 12/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4150 - g_loss: 1.9312 - mean_real_logits: 0.2192 - mean_fake_logits: -1.3045 - mean_gen_logits: -1.9312\n",
      "Epoch 13/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4359 - g_loss: 1.8143 - mean_real_logits: 0.3152 - mean_fake_logits: -1.2270 - mean_gen_logits: -1.8143\n",
      "Epoch 14/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4114 - g_loss: 1.9426 - mean_real_logits: 0.1617 - mean_fake_logits: -1.3575 - mean_gen_logits: -1.9426\n",
      "Epoch 15/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.4036 - g_loss: 1.9820 - mean_real_logits: 0.1177 - mean_fake_logits: -1.3903 - mean_gen_logits: -1.9820\n",
      "Epoch 16/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3843 - g_loss: 1.5358 - mean_real_logits: 0.5462 - mean_fake_logits: -0.9432 - mean_gen_logits: -1.5358\n",
      "Epoch 17/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3986 - g_loss: 1.8079 - mean_real_logits: 0.3127 - mean_fake_logits: -1.1895 - mean_gen_logits: -1.8079\n",
      "Epoch 18/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3806 - g_loss: 2.5415 - mean_real_logits: -0.4833 - mean_fake_logits: -1.9676 - mean_gen_logits: -2.5415\n",
      "Epoch 19/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3829 - g_loss: 1.5180 - mean_real_logits: 0.5608 - mean_fake_logits: -0.9251 - mean_gen_logits: -1.5180\n",
      "Epoch 20/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3843 - g_loss: 1.5201 - mean_real_logits: 0.5150 - mean_fake_logits: -0.9727 - mean_gen_logits: -1.5201\n",
      "Epoch 21/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3754 - g_loss: 1.5176 - mean_real_logits: 0.5741 - mean_fake_logits: -0.9034 - mean_gen_logits: -1.5176\n",
      "Epoch 22/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3721 - g_loss: 1.6774 - mean_real_logits: 0.3727 - mean_fake_logits: -1.0999 - mean_gen_logits: -1.6774\n",
      "Epoch 23/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3606 - g_loss: 1.5266 - mean_real_logits: 0.5209 - mean_fake_logits: -0.9394 - mean_gen_logits: -1.5266\n",
      "Epoch 24/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3555 - g_loss: 1.3659 - mean_real_logits: 0.6490 - mean_fake_logits: -0.8052 - mean_gen_logits: -1.3659\n",
      "Epoch 25/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3484 - g_loss: 1.4049 - mean_real_logits: 0.5996 - mean_fake_logits: -0.8483 - mean_gen_logits: -1.4049\n",
      "Epoch 26/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3490 - g_loss: 1.6371 - mean_real_logits: 0.3833 - mean_fake_logits: -1.0643 - mean_gen_logits: -1.6371\n",
      "Epoch 27/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3487 - g_loss: 1.2922 - mean_real_logits: 0.7036 - mean_fake_logits: -0.7427 - mean_gen_logits: -1.2922\n",
      "Epoch 28/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3401 - g_loss: 1.6107 - mean_real_logits: 0.4149 - mean_fake_logits: -1.0231 - mean_gen_logits: -1.6107\n",
      "Epoch 29/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3419 - g_loss: 1.1534 - mean_real_logits: 0.8296 - mean_fake_logits: -0.6095 - mean_gen_logits: -1.1534\n",
      "Epoch 30/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3463 - g_loss: 0.8177 - mean_real_logits: 1.1759 - mean_fake_logits: -0.2656 - mean_gen_logits: -0.8177\n",
      "Epoch 31/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3350 - g_loss: 1.6059 - mean_real_logits: 0.3667 - mean_fake_logits: -1.0668 - mean_gen_logits: -1.6059\n",
      "Epoch 32/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3391 - g_loss: 1.5022 - mean_real_logits: 0.4945 - mean_fake_logits: -0.9401 - mean_gen_logits: -1.5022\n",
      "Epoch 33/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3315 - g_loss: 1.5328 - mean_real_logits: 0.4231 - mean_fake_logits: -1.0043 - mean_gen_logits: -1.5328\n",
      "Epoch 34/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3201 - g_loss: 1.3703 - mean_real_logits: 0.5957 - mean_fake_logits: -0.8179 - mean_gen_logits: -1.3703\n",
      "Epoch 35/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3249 - g_loss: 1.5873 - mean_real_logits: 0.3774 - mean_fake_logits: -1.0424 - mean_gen_logits: -1.5873\n",
      "Epoch 36/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3185 - g_loss: 1.4652 - mean_real_logits: 0.4838 - mean_fake_logits: -0.9312 - mean_gen_logits: -1.4652\n",
      "Epoch 37/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3137 - g_loss: 1.4335 - mean_real_logits: 0.4912 - mean_fake_logits: -0.9163 - mean_gen_logits: -1.4335\n",
      "Epoch 38/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3128 - g_loss: 1.1036 - mean_real_logits: 0.8486 - mean_fake_logits: -0.5576 - mean_gen_logits: -1.1036\n",
      "Epoch 39/75\n",
      "359/359 [==============================] - 137s 379ms/step - d_loss: -1.3236 - g_loss: 1.2357 - mean_real_logits: 0.6923 - mean_fake_logits: -0.7257 - mean_gen_logits: -1.2357\n",
      "Epoch 40/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3164 - g_loss: 1.5819 - mean_real_logits: 0.3590 - mean_fake_logits: -1.0512 - mean_gen_logits: -1.5819\n",
      "Epoch 41/75\n",
      "359/359 [==============================] - 137s 379ms/step - d_loss: -1.3031 - g_loss: 1.1805 - mean_real_logits: 0.7185 - mean_fake_logits: -0.6767 - mean_gen_logits: -1.1805\n",
      "Epoch 42/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2888 - g_loss: 0.8247 - mean_real_logits: 1.0805 - mean_fake_logits: -0.3002 - mean_gen_logits: -0.8247\n",
      "Epoch 43/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3001 - g_loss: 1.0844 - mean_real_logits: 0.8384 - mean_fake_logits: -0.5535 - mean_gen_logits: -1.0844\n",
      "Epoch 44/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3076 - g_loss: 1.2259 - mean_real_logits: 0.6773 - mean_fake_logits: -0.7235 - mean_gen_logits: -1.2259\n",
      "Epoch 45/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2882 - g_loss: 0.5806 - mean_real_logits: 1.3357 - mean_fake_logits: -0.0448 - mean_gen_logits: -0.5806\n",
      "Epoch 46/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2947 - g_loss: 1.1752 - mean_real_logits: 0.7125 - mean_fake_logits: -0.6739 - mean_gen_logits: -1.1752\n",
      "Epoch 47/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3128 - g_loss: 1.0981 - mean_real_logits: 0.7973 - mean_fake_logits: -0.6083 - mean_gen_logits: -1.0981\n",
      "Epoch 48/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3134 - g_loss: 1.1254 - mean_real_logits: 0.7715 - mean_fake_logits: -0.6338 - mean_gen_logits: -1.1254\n",
      "Epoch 49/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2929 - g_loss: 1.1550 - mean_real_logits: 0.7430 - mean_fake_logits: -0.6398 - mean_gen_logits: -1.1550\n",
      "Epoch 50/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2986 - g_loss: 0.6328 - mean_real_logits: 1.2600 - mean_fake_logits: -0.1289 - mean_gen_logits: -0.6328\n",
      "Epoch 51/75\n",
      "359/359 [==============================] - 137s 379ms/step - d_loss: -1.2905 - g_loss: 0.4965 - mean_real_logits: 1.3928 - mean_fake_logits: 0.0118 - mean_gen_logits: -0.4965\n",
      "Epoch 52/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2931 - g_loss: 0.7592 - mean_real_logits: 1.0934 - mean_fake_logits: -0.2893 - mean_gen_logits: -0.7592\n",
      "Epoch 53/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2995 - g_loss: 0.4477 - mean_real_logits: 1.4360 - mean_fake_logits: 0.0468 - mean_gen_logits: -0.4477\n",
      "Epoch 54/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2961 - g_loss: 0.6993 - mean_real_logits: 1.1524 - mean_fake_logits: -0.2339 - mean_gen_logits: -0.6993\n",
      "Epoch 55/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2907 - g_loss: 0.3864 - mean_real_logits: 1.5220 - mean_fake_logits: 0.1402 - mean_gen_logits: -0.3864\n",
      "Epoch 56/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2925 - g_loss: 0.5566 - mean_real_logits: 1.3328 - mean_fake_logits: -0.0499 - mean_gen_logits: -0.5566\n",
      "Epoch 57/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2983 - g_loss: 0.4010 - mean_real_logits: 1.4810 - mean_fake_logits: 0.0911 - mean_gen_logits: -0.4010\n",
      "Epoch 58/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2951 - g_loss: 0.2504 - mean_real_logits: 1.6282 - mean_fake_logits: 0.2437 - mean_gen_logits: -0.2504\n",
      "Epoch 59/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2982 - g_loss: 0.6315 - mean_real_logits: 1.2201 - mean_fake_logits: -0.1682 - mean_gen_logits: -0.6315\n",
      "Epoch 60/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2965 - g_loss: 0.5828 - mean_real_logits: 1.2822 - mean_fake_logits: -0.1040 - mean_gen_logits: -0.5828\n",
      "Epoch 61/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2772 - g_loss: 0.5987 - mean_real_logits: 1.2784 - mean_fake_logits: -0.0870 - mean_gen_logits: -0.5987\n",
      "Epoch 62/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.3080 - g_loss: 0.4984 - mean_real_logits: 1.3948 - mean_fake_logits: -0.0031 - mean_gen_logits: -0.4984\n",
      "Epoch 63/75\n",
      "359/359 [==============================] - 136s 379ms/step - d_loss: -1.2942 - g_loss: 0.7868 - mean_real_logits: 1.0741 - mean_fake_logits: -0.3084 - mean_gen_logits: -0.7868\n",
      "Epoch 64/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.2963 - g_loss: 0.7791 - mean_real_logits: 1.0741 - mean_fake_logits: -0.3118 - mean_gen_logits: -0.7791\n",
      "Epoch 65/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3140 - g_loss: 0.4562 - mean_real_logits: 1.4096 - mean_fake_logits: 0.0059 - mean_gen_logits: -0.4562\n",
      "Epoch 66/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3073 - g_loss: -0.0406 - mean_real_logits: 1.9190 - mean_fake_logits: 0.5219 - mean_gen_logits: 0.0406\n",
      "Epoch 67/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3165 - g_loss: 0.2576 - mean_real_logits: 1.6055 - mean_fake_logits: 0.1989 - mean_gen_logits: -0.2576\n",
      "Epoch 68/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3115 - g_loss: 0.1346 - mean_real_logits: 1.7423 - mean_fake_logits: 0.3412 - mean_gen_logits: -0.1346\n",
      "Epoch 69/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3165 - g_loss: -0.3260 - mean_real_logits: 2.2007 - mean_fake_logits: 0.7950 - mean_gen_logits: 0.3260\n",
      "Epoch 70/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3105 - g_loss: 0.0019 - mean_real_logits: 1.8701 - mean_fake_logits: 0.4697 - mean_gen_logits: -0.0019\n",
      "Epoch 71/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3161 - g_loss: 0.5521 - mean_real_logits: 1.3071 - mean_fake_logits: -0.0994 - mean_gen_logits: -0.5521\n",
      "Epoch 72/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3164 - g_loss: 0.3656 - mean_real_logits: 1.5186 - mean_fake_logits: 0.1104 - mean_gen_logits: -0.3656\n",
      "Epoch 73/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3256 - g_loss: -0.1373 - mean_real_logits: 2.0039 - mean_fake_logits: 0.5874 - mean_gen_logits: 0.1373\n",
      "Epoch 74/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3268 - g_loss: 0.4041 - mean_real_logits: 1.4767 - mean_fake_logits: 0.0583 - mean_gen_logits: -0.4041\n",
      "Epoch 75/75\n",
      "359/359 [==============================] - 137s 380ms/step - d_loss: -1.3232 - g_loss: 0.2200 - mean_real_logits: 1.6359 - mean_fake_logits: 0.2224 - mean_gen_logits: -0.2200\n",
      "Model saved at save_4_26-02-2021_14-57\n"
     ]
    }
   ],
   "source": [
    "gan.one_growth_cycle()\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22973 files belonging to 1 classes.\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 137s 378ms/step - d_loss: -1.3259 - g_loss: 0.2730 - mean_real_logits: 1.6149 - mean_fake_logits: 0.1988 - mean_gen_logits: -0.2730\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 138s 384ms/step - d_loss: -1.3074 - g_loss: 0.0539 - mean_real_logits: 1.8210 - mean_fake_logits: 0.4249 - mean_gen_logits: -0.0539\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3187 - g_loss: -0.0128 - mean_real_logits: 1.8729 - mean_fake_logits: 0.4656 - mean_gen_logits: 0.0128\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3348 - g_loss: 0.3111 - mean_real_logits: 1.5933 - mean_fake_logits: 0.1691 - mean_gen_logits: -0.3111\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3321 - g_loss: 0.2962 - mean_real_logits: 1.5707 - mean_fake_logits: 0.1490 - mean_gen_logits: -0.2962\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3362 - g_loss: 0.2793 - mean_real_logits: 1.5861 - mean_fake_logits: 0.1600 - mean_gen_logits: -0.2793\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3315 - g_loss: -0.2514 - mean_real_logits: 2.1317 - mean_fake_logits: 0.7100 - mean_gen_logits: 0.2514\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3406 - g_loss: -0.0980 - mean_real_logits: 1.9578 - mean_fake_logits: 0.5272 - mean_gen_logits: 0.0980\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3400 - g_loss: 0.4298 - mean_real_logits: 1.4554 - mean_fake_logits: 0.0242 - mean_gen_logits: -0.4298\n",
      "Epoch 85/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3457 - g_loss: -0.0972 - mean_real_logits: 1.9612 - mean_fake_logits: 0.5260 - mean_gen_logits: 0.0972\n",
      "Epoch 86/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3483 - g_loss: -0.1754 - mean_real_logits: 2.0540 - mean_fake_logits: 0.6152 - mean_gen_logits: 0.1754\n",
      "Epoch 87/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3533 - g_loss: -0.0191 - mean_real_logits: 1.9142 - mean_fake_logits: 0.4691 - mean_gen_logits: 0.0191\n",
      "Epoch 88/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3442 - g_loss: -0.5061 - mean_real_logits: 2.3790 - mean_fake_logits: 0.9448 - mean_gen_logits: 0.5061\n",
      "Epoch 89/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3590 - g_loss: -0.4078 - mean_real_logits: 2.2805 - mean_fake_logits: 0.8300 - mean_gen_logits: 0.4078\n",
      "Epoch 90/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3674 - g_loss: -0.7323 - mean_real_logits: 2.6372 - mean_fake_logits: 1.1778 - mean_gen_logits: 0.7323\n",
      "Epoch 91/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3606 - g_loss: -0.0955 - mean_real_logits: 1.9995 - mean_fake_logits: 0.5461 - mean_gen_logits: 0.0955\n",
      "Epoch 92/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3527 - g_loss: -0.3449 - mean_real_logits: 2.2323 - mean_fake_logits: 0.7903 - mean_gen_logits: 0.3449\n",
      "Epoch 93/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3672 - g_loss: 0.1101 - mean_real_logits: 1.7694 - mean_fake_logits: 0.3118 - mean_gen_logits: -0.1101\n",
      "Epoch 94/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3675 - g_loss: 0.2301 - mean_real_logits: 1.6497 - mean_fake_logits: 0.1912 - mean_gen_logits: -0.2301\n",
      "Epoch 95/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3791 - g_loss: 0.1285 - mean_real_logits: 1.7852 - mean_fake_logits: 0.3131 - mean_gen_logits: -0.1285\n",
      "Epoch 96/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3834 - g_loss: -0.3369 - mean_real_logits: 2.2708 - mean_fake_logits: 0.7951 - mean_gen_logits: 0.3369\n",
      "Epoch 97/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3863 - g_loss: -0.2944 - mean_real_logits: 2.1853 - mean_fake_logits: 0.7055 - mean_gen_logits: 0.2944\n",
      "Epoch 98/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.4029 - g_loss: 0.1457 - mean_real_logits: 1.7679 - mean_fake_logits: 0.2698 - mean_gen_logits: -0.1457\n",
      "Epoch 99/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.4008 - g_loss: -0.0534 - mean_real_logits: 1.9436 - mean_fake_logits: 0.4484 - mean_gen_logits: 0.0534\n",
      "Epoch 100/100\n",
      "359/359 [==============================] - 135s 374ms/step - d_loss: -1.3909 - g_loss: 0.1274 - mean_real_logits: 1.7850 - mean_fake_logits: 0.2994 - mean_gen_logits: -0.1274\n",
      "Model saved at saves/save_4_26-02-2021_16-56\n"
     ]
    }
   ],
   "source": [
    "gan = WGAN(checkpoint_path='save_4_26-02-2021_14-57')\n",
    "gan.fit_n_epochs(100, 75)\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.fit_n_epochs(150, 100)\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.one_growth_cycle()\n",
    "gan.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.one_growth_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QVubRyHG7rkI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63569 files belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"105.385496pt\" version=\"1.1\" viewBox=\"0 0 368.925 105.385496\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-02-19T15:19:09.114622</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 105.385496 \n",
       "L 368.925 105.385496 \n",
       "L 368.925 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 81.507371 \n",
       "L 99.707609 81.507371 \n",
       "L 99.707609 8.724762 \n",
       "L 26.925 8.724762 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p2d5f9c8acb)\">\n",
       "    <image height=\"73\" id=\"imagecd9748b1f6\" transform=\"scale(1 -1)translate(0 -73)\" width=\"73\" x=\"26.925\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAFgElEQVR4nO2ca2xTZRzGz+lOL2u3ruu6CwKbgEMum8AcjJiMgIpmiRATEjGA0SwaxMQErwRM4BOSSKJIDMgXQYKQeInCtuAFcAuoJBjmBBYcowTYYCpd2dZ23XZ66uenzz784+f/8+3557emffrynvd9/28xP9q9K2vkaO1zz4Jfv3ZdLmLEYvfAHzz0GTGJ8irwM0tLiDnYuh98ZcYkpj4YoVpLaAy81/AR0/XXJfDbVr9OTPOTTeAjhQFiXFRRkTQkgTQkgTQkgSwr61Dxi8MHwd+9dZ2YVzc2g9/+9jvEtP7SCX7CmSCmLDAFvCt6i5jAxH2qPZbC7/f3s78R09N7Fd9PUTUxmza/Bf7zT/cSoyNJIA1JIA1JIOtG959UnF6F88R3J04Q0/ptC/h78UFisgbOd2eOHyXm5UVz8W/m87yRHh6j2rX2dvCL58wn5qsz58C/uXYTMZlxG/ze3e8ToyNJIA1JIA1JIA1JIKunu5uKff/gDr+kvIKYtrbj4EfSo8RMRM+DX7FoFjGZTB749IUbxHgeraJaY9NT4KOdXcTYZgZ829fHiLkX+xf8zl3bidGRJJCGJJCGJJBlmHQwadzs6QG/8uk9xPzagRvKjUsWEONx4XeQtfKIsb1Y8zXOJiZvPEO1nKnMmNuwkBhnDD/bN0cPE/PGlm3gg4VBYnQkCaQhCaQhCaQhCWQNJ1NUDOa0VfZ9+DExly7ntGu2biZm2bYt4Nv3HyDGMfGkwGvzg8Qx+BQgOzAMPhXIJya/0A/+0hVeOP9w8iT4dS+8RIyOJIE0JIE0JIEsO4/byiU+/Ld8uuNnYhLpcfBTy8qJ6XXhxtg2eDHpGcM5yeG3Y5j5Hqq9+wdungvWLCfmkaX14LvOXSCmO2djbL3C7XIdSQJpSAJpSAJpSAJZNdUPU3FsNA1+Vi23ay5ewVbUnkMtxLiGvOA9o7xwzWTxAfDj2bPELKtroNqR1svgkx98SczqhplYyOMHR3xkCLw7UEyMjiSBNCSBNCSBNCSBzDXL62jb7csvBG/b47mIEQjhSUFf721ijn2yD3y4gi9t2g724jesf5GYhsYV/Nr9+LYHJlmqL0xFwQ/f4rtP4WL8rE80ryJGR5JAGpJAGpJAVqSIF0/xRAJ8wMenfiVZnF+ClVOICUVy5rYU35ncuuM98OFJWuqrljZSbc7tm+AHU2lijrTgSYHpdhMTDoXA945fJEZHkkAakkAakkAakkBWTc08Kn5/ugN85TSeTK9ew8Xjg0V+Ys53tIOvnlZJjMuPD47ZkxzVTquaSrWdB/DXTU6G7wuUl+CvouIjCWL8bnwo5fn5npWOJIE0JIE0JIHMHa9toA2u7WCbp66W7x61nvoJvNfF7enAGLanMwW8wZ3A/a1hp3neKPOHqeYpLMDXmeTUMZnAVvhAfIiYuQ/NAO99PkmMjiSBNCSBNCSBNCSBJrnqaRgzpuPl8nCQL1uWFmGttDhEzNXr+IvLxxctIeZuH54WOpN8b9ksLxQDxbgIjUb5knyBB1taD1SUEpNK48MlEiwiRkeSQBqSQBqSQFZT0zNUjP19B/z1/jvEFPtxQxtwe4mJBHDecrI2MW4LN7TJJC8m6xbXU62zE38pPprgv6uoxHkrHbtPTH8ffrbpycXE6EgSSEMSSEMSSEMSyGqzolQs64uB9xjcQi4rw4WZM8Htonlz8BdHAR9f2jTDuHgrKOD2VTrN7SIz55Lqghq+Q+XkHDF4y/nhEk+NgE8N6sXS/yUNSSANSSBrpVlLxVMh7JaUxHgR6HfjItDr5a1yKKeFPjQYI8bIuWzv9/HpZTwep1p5Kb62y+R503bhe8r38elpKIKX9DM3+b9K0pEkkIYkkIYkkIYk0H83WG/DIZ7BGQAAAABJRU5ErkJggg==\" y=\"-8.507371\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m32e1bc0804\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"29.199457\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(26.018207 96.105808)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"74.688587\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(68.326087 96.105808)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mb83fc762e5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb83fc762e5\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb83fc762e5\" y=\"56.488349\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(7.2 60.287568)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 81.507371 \n",
       "L 26.925 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 99.707609 81.507371 \n",
       "L 99.707609 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 81.507371 \n",
       "L 99.707609 81.507371 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 8.724762 \n",
       "L 99.707609 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 114.26413 81.507371 \n",
       "L 187.046739 81.507371 \n",
       "L 187.046739 8.724762 \n",
       "L 114.26413 8.724762 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p31007c0773)\">\n",
       "    <image height=\"73\" id=\"imagea7d74aeab6\" transform=\"scale(1 -1)translate(0 -73)\" width=\"73\" x=\"114.26413\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAFa0lEQVR4nO1bbUyVZRh+3/O+53AAjQIHHhKNcqvNUgtcQGRLyKmbk6ZZtjRbyxVzrXQ6R05qrdI5c/3JtmzNpXOVH2A6MbL5EQTyMYhhoM4KRSaiyEHOgfP19vt6r/Pjnr/v699173qec8713rvP89zP85pHllY5hgvbbvYC37XxM7fEKL/cDPybxrOkyV1XDTzqTyONz7KAWx6bNKbho1jcSQD3+rykiUWDwEuL/KT5dOvnwG1zJmk8FFEQ1CQB1CQB1CQB7NEI1W3DisaA32pqJs2xmkPALwWHSJNdOQm4z+ainObDz7dMLsCmYVIsGsdxdfUnSVO2cBHwhqZe0vTfuAbcmZggjWaSAGqSAGqSAPaEkaCgY+MCb/jkYdJkGunAF1e+RRoLS5KRkcY1yZ+G9c+yuf7EY1w3o3Gc6/nieTy3jTlwLzyVNDt37wDef3WQNJpJAqhJAqhJAqhJAthxj8VBCxd0ztilJCNnATXLi0kyIw8rd36AF4oJPxZl2+HnFovyxw+PuBahdjZp2jpw8Zib9yjP41oEF5U8RRrNJAHUJAHUJAHsu54QBR80cZM34uUF53+THgD+cIi7h49NxxoUsbi4eB0cZ5pJNtxeHpczBcdFYzHSzJmFXcZb90hijI2l4nec4M/XTBJATRJATRJATRLA/uf2VQpO9uMicPBmkDT5X3wIfOTUz6Q5HHwBeMUKXswlXIXa4/CfhGkme5Y4LjOLF8Wjo9hRSBnnrmM4jNW8uamPNJpJAqhJAqhJAti3nSSnE1f6gWc8u5A0j9QfA56fkUGaE7u/BD44/yvSBHLwOZkGdy+dJHXKcNWy82fbSZI7bS5wX5KFamFRHvBtm2pJo5kkgJokgJokgJokgJ16i4+nI7Fx4Cuy+V6PPzgM/KFpmaRZMhu7l0erd5Jm3Z4q4D5PkiOuJIU7GsNYPMrPO+pqafZ0tZHmifyngV/svE4azSQB1CQB1CQB1CQBzH2BMlqGxp58HPj1zgYaOLsENZHQbdIUvvEB8BkrF/A3sLB9aiS5C2AkuDXbP4B/LllT00nT3nIXNek8z5SsFOBrX99OGs0kAdQkAdQkAezw8uco2FSHF9dzSlmz7PtdwBOpXEvcDQZPIskzcSI4j8FdiWiSe52tLVeABwIB0qR4saMwHh4jzU8HzwGPTfDxlWaSAGqSAGqSAGqSAPZgYDoFt54+CLytu4c0J841Ap9TVEiaVNczuDMcJk3vv3eAd3W1kOa99a9SrOMytmvvBAtIkzcN5+5u5R3+qV86gFse/gPSTBJATRJATRLAvDYwRCu1VyoqgNecOMoDfbgx7em6TJo/2zuBv/M215bKDR8DX/PaMtI0NjRS7M21a4DX1f5BmvXvLgW+qKySNLEQbow9Jh+XayYJoCYJoCYJoCYJYP729Q9UuD0zc4F/t2cvDfx2/z7gDa38duVfPbhT7+vjuz+rV2Ex/6TqI9K8v3kLxZ4pwEvpPi9fbE334E8rnruaNJP9+Fu9Pi3c9wU1SQA1SQDz7tBNqknbXTVo+WK+n/Tr8XrgG7ds4MlddeJC0wXSbN60CXjZgnLSFJS9SLHgDewgrlzFmhQXf/mlatJEQtgZdUx9m/u+oCYJoCYJoCYJYDsDnRScGMIOXu3xOtIsLS8D/ncnzzM2jkfRudl87FNYgPeDCktLSDN/Hncdz5zFroMdHyfNgf3ngZtJ3sq0Lbx7tfdAFWk0kwRQkwRQkwSwQ+EIBU3XffPxCB/9Hjp+BPjKJRWk8bpenAmN8VuaO3bicfnh30+Tpvsin3JkZU0GnkjwUXjNj2eAO3HeBJse/I7B0ChpNJMEUJMEUJMEUJME+B+vEn/HghWjpQAAAABJRU5ErkJggg==\" y=\"-8.507371\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.538587\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(113.357337 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.027717\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(155.665217 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.26413\" xlink:href=\"#mb83fc762e5\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(100.90163 14.798438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.26413\" xlink:href=\"#mb83fc762e5\" y=\"56.488349\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(94.53913 60.287568)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 114.26413 81.507371 \n",
       "L 114.26413 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 187.046739 81.507371 \n",
       "L 187.046739 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 114.26413 81.507371 \n",
       "L 187.046739 81.507371 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 114.26413 8.724762 \n",
       "L 187.046739 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 201.603261 81.507371 \n",
       "L 274.38587 81.507371 \n",
       "L 274.38587 8.724762 \n",
       "L 201.603261 8.724762 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pcebc038a57)\">\n",
       "    <image height=\"73\" id=\"image2e8d19650d\" transform=\"scale(1 -1)translate(0 -73)\" width=\"73\" x=\"201.603261\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAFdklEQVR4nO2c229UVRjFz5k502FmOi1jb7S2QGkploaCibYlqTwQo8FIIIYYEoJRwfBijCZE8Q9QIyReiJIgBlOKEojiBTSagpYYmlKLlGkqCr3Ye6czpbXXmbZzxud11jx84flbb+vLb2ZOV3e/s2fvfWpGI8Mpw6FkfBp8bq7biRi2mQe+t/MfYkrWbwD/5MGj/D6BVeB9hYXETI2PUs09hbXjbx4gpvbRXPCDw7P8PpcvgU9tqSfGRRUVSUMSSEMSSEMSyEwt29S4IwPYhMM3O+iF9du3gx+5x417ZQ424RfO3CHGX1oOPjbYTUysvZVqthFHP/MfMX9d/Rh8WzvfADyREfCuht+I0ZEkkIYkkIYkkLWcjFPxbrgTfGwkRsxwTy/4iegMMa3dk+Dv3z5HzKlX3wPvC24l5uD7I1TruDUEPmDxz7G0aIO/ePEsMY+VlYGv7beJ0ZEkkIYkkIYkkIYkkDnUcYMmk21tYfDLLl4FmJ1dAB/0c8PbtX8neNvi34llYi2VNIlJ8/GGuYDf6Cda/+BrLN4C3u1Y3TAMw7h6Ayevu0708+fzx6uc0pAE0pAEssZGI1SsqsIVxXvdA8TMebB31NUVEJMcx0mgt6iYmWQSvNudpgEZ1DaNlDcTfE7VGmK8iQD4rm7+Eu7zZICfMZaI0ZEkkIYkkIYkkIYkkDUS4W2WfJcf/FKSm2nNOh/4K2+3ELP7jZfww1ZxA3Y75qA2zyUN0+aJ6pzzpuAOEjP4N17TYB+vXi77Q44L0sb9QNKQBNKQBDLnJqepUZw9+Tn4yopKemEkjr3MdayPmJiJk9ADbW8R40o5+k3SIsZO86t87ZlXwH/09SfELExEwTddGyTG48YVzfXHw8ToSBJIQxJIQxJIQxLIWkwuU7G0rBR8wubtml9O47me6VmeKJ66fhi8K8HvExueAJ+7ZjUx3+3/gmoVPXiN5/Y0ELP7g73g5+cniMn2rQA/lUoSoyNJIA1JIA1JIA1JICu+xM00lcDtooWZSWKOnsTZ80QXn+txR++DvxkeJibD8fmZOT5iyl/eSbWh8FfgZ+6mubmcbwJvZ/NNygjiMvDMPK+K6EgSSEMSSEMSyIxFozQLDD2UA/5y43l6YbAAt4c2h/h8Ukcv9jI7I0BMiXcKfP66EmK2bnqaasWrt4HPNHlJ052LPfHQ64eJmbewJ41++jMxOpIE0pAE0pAE0pAEsgI+DxXjizgx8zj2yw3DMMYcS6PuCJ/9WVqRBX77szuIabmAhz1zQxuIaTz9LtVK63eBt8c7iekZw6Xg7ihfY+SbK+CrDz1FjI4kgTQkgTQkgSyPl/uN4cLs8otyCJnrw4naQoC3mVflYe335uvEZHlxSym4MosYT5DPPoWKcTJ768/bxMwv4xfaQIafGN8kToIzAzxudCQJpCEJpCEJpCEJZLlMzsl2HOSsqK4gZmwMn2asfaKWmKwQNu4fzv9ITPk2fALzzIfHiSlaXUq16hSemRqJ85aWy9G4W058T8yeI/vATyd0S+mBpCEJpCEJZBkp/ls2FrHmD2YTstLRbzLzecKZSuFqYU3NJmLySvCQeiift7l37N1DtW8bL4Bfs5YP0kfDeB7pxX3PE7NxB/bEa1eaidGRJJCGJJCGJJCGJJBlpzm1ORf9F3yweC0xGzfjv89wpcnbNnFiVri+nBjDxMPlj9emYdy8XZTlx+2pxABvaRX04Zb1I8f4BvDTlxfBO88rGYaOJJE0JIE0JIHM9nca+GnuX5vBH2r6jF9o4Ipi0kz3cI3jwR2TH+RZnJ0Cn+HnrfC+ri6q9V/uAP+w7SWm7Mhz4O80NxMzPZ8AX1lTR4yOJIE0JIE0JIE0JIH+B1uhjrtQPsshAAAAAElFTkSuQmCC\" y=\"-8.507371\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.877717\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(200.696467 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.366848\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(243.004348 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.603261\" xlink:href=\"#mb83fc762e5\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(188.240761 14.798438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.603261\" xlink:href=\"#mb83fc762e5\" y=\"56.488349\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(181.878261 60.287568)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 201.603261 81.507371 \n",
       "L 201.603261 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 274.38587 81.507371 \n",
       "L 274.38587 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 201.603261 81.507371 \n",
       "L 274.38587 81.507371 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 201.603261 8.724762 \n",
       "L 274.38587 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 288.942391 81.507371 \n",
       "L 361.725 81.507371 \n",
       "L 361.725 8.724762 \n",
       "L 288.942391 8.724762 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p4b5e95e652)\">\n",
       "    <image height=\"73\" id=\"image8e0d6d5a4b\" transform=\"scale(1 -1)translate(0 -73)\" width=\"73\" x=\"288.942391\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAFSUlEQVR4nO1by2tcdRS+v7l33pPOTGamM9NpTB+mmhjFjfEB1QouilrShdgoltqsKpUKgi5EBUXp2j+g7gpCRVooCK2bVGKLROyD0jZ1aPNOOpNJM5lkHvfl+vy+WRy6Pt/u+/HdMPlycub8zjlX+b7rGxp+/PoLwq9NTugS49j4UcIvXkSNGbYIP/39V6BJmzbhyzPzoLnwx19w9vvVfwmvb7VB47m2xkFitFotwj89dRI0AXxMoENMYkBMYkBMYsAyfMjbRrvdIXxg4Gl80qdJebVaA0ksbBK+dG8aNA1FE25tuQqa6es34SxsO4RnM2nQjIyMEJ7P5kCjLEX48fFx0EgkMSAmMSAmMWC5dgcOl1cWCe8vlUDjux498GzQ1FYrhLsh/ACloWHCe/sboIlP/g1nG5V79GCtCZqPjowSntvRC5rffj1PuO85oJFIYkBMYkBMYkBMYsC6ewcLtaBJvbO7JPe1tVXCDx96CzTHjn9AuGvgNbyj3dRj+Sxovjv9DZytVOmXwqkTn4Mmn6TfFEETf48Xn6eFsuugRiKJATGJATGJATVx6TzccB+U7xI+OTEJDw4ODRB+8sRR0AQCNN+YZhQ0jnbBNgMWaDx7C858j3YUHy1tgKZcniN8/9sHQWN3aA66NnUbNBJJDIhJDIhJDIhJDFg96Tgc9iS3EZ7LZUBz4JU3Cf/hp59Bs++ZvYR/+N4oaIKONgqKREBTfYTjorPnLhPuN1GTNOgXx/53D4PGULR70BM1QSKRxICYxICYxIBafHALislbN6cIr1RW4MFzv1wg/PbSKmgcn04ibkxdAU0ijNMaHVM3/oOz0TE61RjYhd3Tz8YOEb67bydo9g72E97wsZiVSGJATGJATGJATGLACgWDcJhMJglfX8PR89jY+4R/8uW3oGl36HjmyqXLoHnn4AF6EFCgOXvmDJzV5ugNf87GLkA0+THhffuGQNO064THYwnQSCQxICYxICYxICYxoFynCiXv1T9pZTx3vwwPui3a9ozFsJuQLNHx0PALmDizaTqf9xwcO9XbuGewMEv3Fab/uQ4aQ9uPSm3Dz/j6G68R3mhh3EgkMSAmMSAmMWA5Nu7jFAtFwlcXl0HjxmgHcWF2Fn94nBaG6Rh2HdtN2hmMhHCJqTKLOdGp0+KxVn8Mmj2DNAcWentA09ii+S5VfBY0EkkMiEkMiEkMiEkMWI7jweFGgybFbBGXxBNap6Baw2X3YDhG+PT9GdBsNjcJf+nVl0Ez/xDfXNrQEneugGOv7YUC4bt3PQWagKW1axUWsxJJDIhJDIhJDFj1TVwuT2VShEfiYdDEQ3TX6LlhvLxGI7QwfFjGorA3Q3PJQhnzVsDDbmWpn46CWi7mknxOW243MSZsbewV9uWC+0QQkxgQkxgQkxhQtr0OnclOm45ZtjbX4UGnQ2/Pto3dw+YmLRS9DnYc5mdo9yDQZTUgGsWF1Fwfnf23uiypF4u0mAxF8Od0XBon6fQO0EgkMSAmMSAmMWDpL0AahmEEtKV038ARciRGNWEfu47NFt1jzJS2g0aZdKJRq+CeU3En7h5ZWpczEcRdR0d/4afLW5HNJjUgmQKJRBIHYhIDYhIDYhIDyvNtKN+Wl2gnMBTGG7YyaFK2O6gxlZZMQ5hcPa0I3XpcB00kiaOgsLYU73lYhSqteaAM3MVKJPKEB0wcaUkkMSAmMSAmMaBcH/+ZlU+LrsUVXDYPBeljTpeVmR5tWtLu8ja3qSeOLtWtZ+HfUmkfu9vKvNJyomvjInsmoy3AK+lMPhHEJAbEJAbEJAb+B/LGiYDaGD5HAAAAAElFTkSuQmCC\" y=\"-8.507371\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.216848\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(288.035598 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.705978\" xlink:href=\"#m32e1bc0804\" y=\"81.507371\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(330.343478 96.105808)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.942391\" xlink:href=\"#mb83fc762e5\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(275.579891 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.942391\" xlink:href=\"#mb83fc762e5\" y=\"56.488349\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(269.217391 60.287568)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 288.942391 81.507371 \n",
       "L 288.942391 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 361.725 81.507371 \n",
       "L 361.725 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 288.942391 81.507371 \n",
       "L 361.725 81.507371 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 288.942391 8.724762 \n",
       "L 361.725 8.724762 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2d5f9c8acb\">\n",
       "   <rect height=\"72.782609\" width=\"72.782609\" x=\"26.925\" y=\"8.724762\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p31007c0773\">\n",
       "   <rect height=\"72.782609\" width=\"72.782609\" x=\"114.26413\" y=\"8.724762\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pcebc038a57\">\n",
       "   <rect height=\"72.782609\" width=\"72.782609\" x=\"201.603261\" y=\"8.724762\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p4b5e95e652\">\n",
       "   <rect height=\"72.782609\" width=\"72.782609\" x=\"288.942391\" y=\"8.724762\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = []\n",
    "\n",
    "for image in get_dataset((16, 16), 4).unbatch().take(4):\n",
    "    images.append((image.numpy() + 1.0) / 2.0)\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal((36, 512))\n",
    "\n",
    "images = gan.generator_func(noise, training=False)\n",
    "images += 1.0\n",
    "images /= 2.0\n",
    "images = (images - tf.reduce_min(images, axis=[1, 2], keepdims=True)) / (tf.reduce_max(images, axis=[1, 2], keepdims=True) - tf.reduce_min(images, axis=[1, 2], keepdims=True))\n",
    "\n",
    "fig, ax = plt.subplots(6, 6, figsize=(20, 20))\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        ax[i][j].imshow(images[i * 6 + j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AniGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
